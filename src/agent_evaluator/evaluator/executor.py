"""è¯„ä¼°æ‰§è¡Œå™¨ï¼ˆè°ƒç”¨ragasï¼‰"""

import asyncio
import time
from typing import Any

from agent_evaluator.core.result import SampleResult
from agent_evaluator.core.sample import EvalSample
from agent_evaluator.utils.logger import get_logger

logger = get_logger(__name__)


class EvaluatorExecutor:
    """è¯„ä¼°æ‰§è¡Œå™¨ï¼Œè´Ÿè´£è°ƒç”¨Ragasè¿›è¡ŒæŒ‡æ ‡è¯„ä¼°"""

    def __init__(self, metrics: list[Any], llm: Any, embeddings: Any | None = None, timeout: float = 120.0):
        """
        åˆå§‹åŒ–è¯„ä¼°æ‰§è¡Œå™¨

        Args:
            metrics: RagasæŒ‡æ ‡åˆ—è¡¨
            llm: è¯„ä¼°ç”¨çš„LLM
            embeddings: è¯„ä¼°ç”¨çš„embeddingsï¼ˆå¯é€‰ï¼‰
            timeout: å•ä¸ªæŒ‡æ ‡è¯„ä¼°çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤120ç§’
        """
        self.metrics = metrics
        self.llm = llm
        self.embeddings = embeddings
        self.timeout = timeout

    async def evaluate(self, eval_sample: EvalSample) -> SampleResult:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬

        Args:
            eval_sample: è¯„ä¼°æ ·æœ¬

        Returns:
            SampleResultå¯¹è±¡
        """
        eval_start_time = time.time()
        try:
            # æ•°æ®éªŒè¯ï¼šç¡®ä¿responseä¸ä¸ºç©º
            if not eval_sample.response or not eval_sample.response.strip():
                logger.warning("å“åº”ä¸ºç©ºï¼Œè·³è¿‡è¯„ä¼°")
                return SampleResult(error="å“åº”ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            
            # è½¬æ¢ä¸ºRagasæ ¼å¼
            logger.debug("æ­£åœ¨è½¬æ¢è¯„ä¼°æ ·æœ¬ä¸ºRagasæ ¼å¼...")
            ragas_sample = eval_sample.to_ragas_single_turn()
            
            # æ˜¾ç¤ºè¯„ä¼°æ ·æœ¬æ‘˜è¦
            logger.info(f"è¯„ä¼°æ ·æœ¬æ‘˜è¦:")
            logger.info(f"  - ç”¨æˆ·è¾“å…¥: {eval_sample.user_input[:100]}{'...' if len(eval_sample.user_input) > 100 else ''}")
            logger.info(f"  - å“åº”é•¿åº¦: {len(eval_sample.response)} å­—ç¬¦")
            logger.info(f"  - ä¸Šä¸‹æ–‡æ•°é‡: {len(ragas_sample.retrieved_contexts)}")
            if ragas_sample.retrieved_contexts:
                # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦ä¸ºç©ºå­—ç¬¦ä¸²
                non_empty_contexts = [ctx for ctx in ragas_sample.retrieved_contexts if ctx and ctx.strip()]
                if len(non_empty_contexts) != len(ragas_sample.retrieved_contexts):
                    logger.warning(f"  âš ï¸ å‘ç°ç©ºä¸Šä¸‹æ–‡: æ€»æ•°={len(ragas_sample.retrieved_contexts)}, éç©º={len(non_empty_contexts)}")
                if non_empty_contexts:
                    contexts_preview = non_empty_contexts[0][:50] if non_empty_contexts[0] else ""
                    logger.info(f"  - ä¸Šä¸‹æ–‡é¢„è§ˆ: {contexts_preview}{'...' if len(contexts_preview) >= 50 else ''}")
                else:
                    logger.warning(f"  âš ï¸ æ‰€æœ‰ä¸Šä¸‹æ–‡éƒ½ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯„ä¼°å¤±è´¥")
            
            # éªŒè¯ragas_sampleçš„å…³é”®å­—æ®µ
            if not ragas_sample.retrieved_contexts or (len(ragas_sample.retrieved_contexts) == 1 and not ragas_sample.retrieved_contexts[0]):
                logger.warning("retrieved_contextsä¸ºç©ºï¼ŒæŸäº›æŒ‡æ ‡å¯èƒ½æ— æ³•æ­£ç¡®è¯„ä¼°")

            # è°ƒç”¨Ragasè¿›è¡Œè¯„ä¼°
            scores: dict[str, float] = {}
            reasoning: dict[str, str] = {}  # å­˜å‚¨è¯„åˆ†ç†ç”±
            errors: dict[str, str] = {}
            total_metrics = len(self.metrics)

            logger.info(f"å¼€å§‹è¯„ä¼°æŒ‡æ ‡ï¼Œå…± {total_metrics} ä¸ªæŒ‡æ ‡")
            for idx, metric in enumerate(self.metrics, 1):
                metric_name = metric.__class__.__name__
                metric_start_time = time.time()
                
                # åœ¨è¯„ä¼°æŒ‡æ ‡ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œä»¥å‡å°‘429é™æµé”™è¯¯
                # æ³¨æ„ï¼šè¿™ä¸ªå»¶è¿Ÿéœ€è¦åœ¨åˆ›å»ºEvaluatorExecutoræ—¶ä¼ å…¥ï¼Œæš‚æ—¶å…ˆä¸å®ç°
                # å¦‚æœé‡åˆ°429é”™è¯¯ï¼Œå»ºè®®åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®request_delay
                
                try:
                    logger.info(f"[{idx}/{total_metrics}] ğŸ”„ æ­£åœ¨è¯„ä¼°æŒ‡æ ‡: {metric_name}...")
                    
                    # åˆ›å»ºä¸€ä¸ªåå°ä»»åŠ¡æ¥å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯10ç§’è¾“å‡ºä¸€æ¬¡ï¼‰
                    async def progress_monitor():
                        while True:
                            await asyncio.sleep(10)  # æ¯10ç§’è¾“å‡ºä¸€æ¬¡
                            elapsed = time.time() - metric_start_time
                            remaining = max(0, self.timeout - elapsed)
                            if remaining > 0:
                                logger.info(f"[{idx}/{total_metrics}] â³ {metric_name} è¯„ä¼°ä¸­... (å·²ç”¨: {elapsed:.1f}ç§’, å‰©ä½™: {remaining:.1f}ç§’)")
                    
                    progress_task = asyncio.create_task(progress_monitor())
                    
                    try:
                        logger.debug(f"[{idx}/{total_metrics}] å¼€å§‹è°ƒç”¨Ragasçš„single_turn_ascoreæ–¹æ³•...")
                        logger.debug(f"[{idx}/{total_metrics}] è¶…æ—¶è®¾ç½®: {self.timeout}ç§’")
                        
                        # è°ƒç”¨Ragasçš„single_turn_ascoreæ–¹æ³•ï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
                        # ragasçš„æ—¥å¿—å·²ç»é€šè¿‡loguruæ‹¦æˆªå™¨ç»Ÿä¸€è¾“å‡ºï¼Œæ— éœ€é¢å¤–é…ç½®
                        score = await asyncio.wait_for(
                            metric.single_turn_ascore(ragas_sample),
                            timeout=self.timeout
                        )
                        progress_task.cancel()  # å®Œæˆåå–æ¶ˆè¿›åº¦ç›‘æ§
                        try:
                            await progress_task
                        except asyncio.CancelledError:
                            pass
                        
                        metric_duration = time.time() - metric_start_time
                        score_value = float(score)
                        scores[metric_name] = score_value
                        
                        # ç”Ÿæˆè¯„åˆ†ç†ç”±ï¼ˆåŸºäºæŒ‡æ ‡ç±»å‹å’Œåˆ†æ•°ï¼‰
                        if metric_name == "Faithfulness":
                            # Faithfulness: 0.0-1.0ï¼Œè¡¨ç¤ºå“åº”ä¸­å¿ å®äºä¸Šä¸‹æ–‡çš„ä¸»å¼ æ¯”ä¾‹
                            if score_value >= 0.8:
                                reason = f"å“åº”é«˜åº¦å¿ å®äºä¸Šä¸‹æ–‡ï¼ˆ{score_value:.1%}çš„ä¸»å¼ å¾—åˆ°æ”¯æŒï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"å“åº”éƒ¨åˆ†å¿ å®äºä¸Šä¸‹æ–‡ï¼ˆ{score_value:.1%}çš„ä¸»å¼ å¾—åˆ°æ”¯æŒï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value > 0:
                                reason = f"å“åº”å¿ å®åº¦è¾ƒä½ï¼ˆä»…{score_value:.1%}çš„ä¸»å¼ å¾—åˆ°æ”¯æŒï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå¯èƒ½å­˜åœ¨å¹»è§‰"
                            else:
                                reason = "å“åº”å®Œå…¨ä¸å¿ å®äºä¸Šä¸‹æ–‡ï¼ˆå¾—åˆ†0.0/1.0ï¼‰ï¼Œå­˜åœ¨ä¸¥é‡å¹»è§‰"
                        elif metric_name == "ResponseRelevancy":
                            # ResponseRelevancy: 0.0-1.0ï¼Œè¡¨ç¤ºå“åº”ä¸é—®é¢˜çš„ç›¸å…³æ€§
                            if score_value >= 0.8:
                                reason = f"å“åº”é«˜åº¦ç›¸å…³ï¼ˆç›¸å…³æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"å“åº”éƒ¨åˆ†ç›¸å…³ï¼ˆç›¸å…³æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå¯èƒ½é—æ¼éƒ¨åˆ†ä¿¡æ¯"
                            elif score_value > 0:
                                reason = f"å“åº”ç›¸å…³æ€§è¾ƒä½ï¼ˆç›¸å…³æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå¯èƒ½æœªå……åˆ†å›ç­”é—®é¢˜"
                            else:
                                reason = "å“åº”ä¸é—®é¢˜ä¸ç›¸å…³ï¼ˆå¾—åˆ†0.0/1.0ï¼‰ï¼Œå¯èƒ½å®Œå…¨åç¦»ä¸»é¢˜"
                        elif metric_name == "ContextPrecision":
                            # ContextPrecision: 0.0-1.0ï¼Œè¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­ä¸é—®é¢˜ç›¸å…³çš„æ¯”ä¾‹
                            if score_value >= 0.8:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡é«˜åº¦ç²¾ç¡®ï¼ˆ{score_value:.1%}çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜ç›¸å…³ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ç²¾ç¡®ï¼ˆ{score_value:.1%}çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜ç›¸å…³ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨æ— å…³ä¸Šä¸‹æ–‡"
                            elif score_value > 0:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦è¾ƒä½ï¼ˆä»…{score_value:.1%}çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜ç›¸å…³ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨è¾ƒå¤šå™ªå£°"
                            else:
                                reason = "æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å®Œå…¨ä¸ç›¸å…³ï¼ˆå¾—åˆ†0.0/1.0ï¼‰ï¼Œæ£€ç´¢è´¨é‡å·®"
                        elif metric_name == "ContextRecall":
                            # ContextRecall: 0.0-1.0ï¼Œè¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡è¦†ç›–æ ‡å‡†ç­”æ¡ˆçš„ç¨‹åº¦
                            if score_value >= 0.8:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡é«˜åº¦å®Œæ•´ï¼ˆè¦†ç›–äº†{score_value:.1%}çš„æ ‡å‡†ç­”æ¡ˆå†…å®¹ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†å®Œæ•´ï¼ˆè¦†ç›–äº†{score_value:.1%}çš„æ ‡å‡†ç­”æ¡ˆå†…å®¹ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œé—æ¼éƒ¨åˆ†ä¿¡æ¯"
                            elif score_value > 0:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å®Œæ•´æ€§è¾ƒä½ï¼ˆä»…è¦†ç›–{score_value:.1%}çš„æ ‡å‡†ç­”æ¡ˆå†…å®¹ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œé—æ¼è¾ƒå¤šä¿¡æ¯"
                            else:
                                reason = "æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å®Œå…¨ä¸åŒ…å«æ ‡å‡†ç­”æ¡ˆå†…å®¹ï¼ˆå¾—åˆ†0.0/1.0ï¼‰ï¼Œæ£€ç´¢å¬å›ç‡ä½"
                        elif metric_name == "ContextEntityRecall":
                            # ContextEntityRecall: 0.0-1.0ï¼Œè¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­åŒ…å«æ ‡å‡†ç­”æ¡ˆä¸­å®ä½“çš„æ¯”ä¾‹
                            if score_value >= 0.8:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åŒ…å«å¤§éƒ¨åˆ†å®ä½“ï¼ˆ{score_value:.1%}çš„æ ‡å‡†ç­”æ¡ˆå®ä½“åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åŒ…å«éƒ¨åˆ†å®ä½“ï¼ˆ{score_value:.1%}çš„æ ‡å‡†ç­”æ¡ˆå®ä½“åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œé—æ¼éƒ¨åˆ†å®ä½“"
                            elif score_value > 0:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å®ä½“è¦†ç›–ç‡è¾ƒä½ï¼ˆä»…{score_value:.1%}çš„æ ‡å‡†ç­”æ¡ˆå®ä½“åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œé—æ¼è¾ƒå¤šå®ä½“"
                            else:
                                reason = "æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸åŒ…å«æ ‡å‡†ç­”æ¡ˆä¸­çš„å®ä½“ï¼ˆå¾—åˆ†0.0/1.0ï¼‰ï¼Œå®ä½“å¬å›ç‡ä½"
                        elif metric_name == "AnswerCorrectness":
                            # AnswerCorrectness: 0.0-1.0ï¼Œè¡¡é‡ç­”æ¡ˆçš„æ­£ç¡®ç¨‹åº¦
                            if score_value >= 0.8:
                                reason = f"ç­”æ¡ˆé«˜åº¦æ­£ç¡®ï¼ˆæ­£ç¡®æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"ç­”æ¡ˆéƒ¨åˆ†æ­£ç¡®ï¼ˆæ­£ç¡®æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨éƒ¨åˆ†é”™è¯¯"
                            elif score_value > 0:
                                reason = f"ç­”æ¡ˆæ­£ç¡®æ€§è¾ƒä½ï¼ˆæ­£ç¡®æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨è¾ƒå¤šé”™è¯¯"
                            else:
                                reason = "ç­”æ¡ˆå®Œå…¨ä¸æ­£ç¡®ï¼ˆå¾—åˆ†0.0/1.0ï¼‰"
                        elif metric_name == "AnswerAccuracy":
                            # AnswerAccuracy: 0.0-1.0ï¼Œè¡¡é‡ç­”æ¡ˆçš„å‡†ç¡®ç¨‹åº¦
                            if score_value >= 0.8:
                                reason = f"ç­”æ¡ˆé«˜åº¦å‡†ç¡®ï¼ˆå‡†ç¡®æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"ç­”æ¡ˆéƒ¨åˆ†å‡†ç¡®ï¼ˆå‡†ç¡®æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨åå·®"
                            elif score_value > 0:
                                reason = f"ç­”æ¡ˆå‡†ç¡®æ€§è¾ƒä½ï¼ˆå‡†ç¡®æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨è¾ƒå¤§åå·®"
                            else:
                                reason = "ç­”æ¡ˆå®Œå…¨ä¸å‡†ç¡®ï¼ˆå¾—åˆ†0.0/1.0ï¼‰"
                        elif metric_name == "ContextRelevance":
                            # ContextRelevance: 0.0-1.0ï¼Œè¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜çš„ç›¸å…³æ€§
                            if score_value >= 0.8:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡é«˜åº¦ç›¸å…³ï¼ˆç›¸å…³æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ç›¸å…³ï¼ˆç›¸å…³æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨æ— å…³å†…å®¹"
                            elif score_value > 0:
                                reason = f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¾ƒä½ï¼ˆç›¸å…³æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨è¾ƒå¤šæ— å…³å†…å®¹"
                            else:
                                reason = "æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜ä¸ç›¸å…³ï¼ˆå¾—åˆ†0.0/1.0ï¼‰"
                        elif metric_name == "ResponseGroundedness":
                            # ResponseGroundedness: 0.0-1.0ï¼Œè¡¡é‡å“åº”åŸºäºä¸Šä¸‹æ–‡çš„ç¨‹åº¦
                            if score_value >= 0.8:
                                reason = f"å“åº”é«˜åº¦åŸºäºä¸Šä¸‹æ–‡ï¼ˆåŸºç¡€æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰"
                            elif score_value >= 0.5:
                                reason = f"å“åº”éƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡ï¼ˆåŸºç¡€æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œå­˜åœ¨æœªåŸºäºä¸Šä¸‹æ–‡çš„å†…å®¹"
                            elif score_value > 0:
                                reason = f"å“åº”åŸºç¡€æ€§è¾ƒä½ï¼ˆåŸºç¡€æ€§å¾—åˆ†: {score_value:.1%}ï¼Œæ»¡åˆ†1.0ï¼‰ï¼Œè¾ƒå¤šå†…å®¹æœªåŸºäºä¸Šä¸‹æ–‡"
                            else:
                                reason = "å“åº”å®Œå…¨ä¸åŸºäºä¸Šä¸‹æ–‡ï¼ˆå¾—åˆ†0.0/1.0ï¼‰ï¼Œå¯èƒ½å­˜åœ¨å¹»è§‰"
                        else:
                            # å…¶ä»–æŒ‡æ ‡çš„é€šç”¨ç†ç”±
                            if score_value >= 0.8:
                                reason = f"å¾—åˆ†è¾ƒé«˜ï¼ˆ{score_value:.4f}/1.0ï¼‰ï¼Œè¡¨ç°è‰¯å¥½"
                            elif score_value >= 0.5:
                                reason = f"å¾—åˆ†ä¸­ç­‰ï¼ˆ{score_value:.4f}/1.0ï¼‰ï¼Œè¡¨ç°ä¸€èˆ¬"
                            elif score_value > 0:
                                reason = f"å¾—åˆ†è¾ƒä½ï¼ˆ{score_value:.4f}/1.0ï¼‰ï¼Œéœ€è¦æ”¹è¿›"
                            else:
                                reason = f"å¾—åˆ†: {score_value:.4f}ï¼ˆæ»¡åˆ†: 1.0ï¼‰ï¼Œè¡¨ç°è¾ƒå·®"
                        
                        reasoning[metric_name] = reason
                        logger.info(f"[{idx}/{total_metrics}] âœ… æŒ‡æ ‡ {metric_name} è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {score_value:.4f}/1.0 (è€—æ—¶: {metric_duration:.2f}ç§’)")
                        logger.info(f"[{idx}/{total_metrics}]   è¯„åˆ†ç†ç”±: {reason}")
                    except asyncio.TimeoutError:
                        progress_task.cancel()
                        try:
                            await progress_task
                        except asyncio.CancelledError:
                            pass
                        metric_duration = time.time() - metric_start_time
                        error_msg = f"è¯„ä¼°è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰"
                        scores[metric_name] = 0.0
                        errors[metric_name] = error_msg
                        logger.warning(f"[{idx}/{total_metrics}] â±ï¸ æŒ‡æ ‡ {metric_name} è¯„ä¼°è¶…æ—¶ (è€—æ—¶: {metric_duration:.2f}ç§’)")
                        logger.warning(f"[{idx}/{total_metrics}] è¶…æ—¶åŸå› åˆ†æï¼š")
                        logger.warning(f"[{idx}/{total_metrics}]   1. LLM APIå“åº”è¿‡æ…¢ï¼ˆå½“å‰è¶…æ—¶è®¾ç½®: {self.timeout}ç§’ï¼‰")
                        logger.warning(f"[{idx}/{total_metrics}]   2. Ragasçš„promptè¾ƒé•¿ï¼Œéœ€è¦æ›´å¤šå¤„ç†æ—¶é—´")
                        logger.warning(f"[{idx}/{total_metrics}]   3. å»ºè®®ï¼šåœ¨é…ç½®æ–‡ä»¶ä¸­å¢åŠ timeoutå€¼ï¼ˆå¦‚180ç§’æˆ–240ç§’ï¼‰")
                        logger.warning(f"[{idx}/{total_metrics}]   4. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIæœåŠ¡çŠ¶æ€")
                    except Exception as e:
                        progress_task.cancel()
                        try:
                            await progress_task
                        except asyncio.CancelledError:
                            pass
                        metric_duration = time.time() - metric_start_time
                        scores[metric_name] = 0.0
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯429é™æµé”™è¯¯
                        error_str = str(e)
                        if "429" in error_str or "Too Many Requests" in error_str:
                            error_msg = f"APIé™æµé”™è¯¯ï¼ˆ429 Too Many Requestsï¼‰: {error_str}"
                            logger.error(f"[{idx}/{total_metrics}] ğŸš« æŒ‡æ ‡ {metric_name} è¯„ä¼°å¤±è´¥ - APIé™æµ")
                            logger.error(f"[{idx}/{total_metrics}] å»ºè®®ï¼š")
                            logger.error(f"[{idx}/{total_metrics}]   1. æ£€æŸ¥APIé…é¢æ˜¯å¦å……è¶³")
                            logger.error(f"[{idx}/{total_metrics}]   2. å‡å°‘å¹¶å‘è¯·æ±‚æ•°é‡")
                            logger.error(f"[{idx}/{total_metrics}]   3. å¢åŠ è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿ")
                            logger.error(f"[{idx}/{total_metrics}]   4. è”ç³»APIæä¾›å•†æå‡é…é¢")
                        else:
                            error_msg = str(e)
                            logger.warning(f"[{idx}/{total_metrics}] âŒ æŒ‡æ ‡ {metric_name} è¯„ä¼°å¤±è´¥ (è€—æ—¶: {metric_duration:.2f}ç§’): {e}")
                        
                        errors[metric_name] = error_msg
                        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                        import traceback
                        logger.debug(f"æŒ‡æ ‡ {metric_name} è¯„ä¼°å¤±è´¥è¯¦æƒ…:\n{traceback.format_exc()}")
                except Exception as e:
                    metric_duration = time.time() - metric_start_time
                    logger.error(f"[{idx}/{total_metrics}] æŒ‡æ ‡ {metric_name} è¯„ä¼°è¿‡ç¨‹å¼‚å¸¸ (è€—æ—¶: {metric_duration:.2f}ç§’): {e}")
                    scores[metric_name] = 0.0
                    errors[metric_name] = str(e)

            # è¯„ä¼°å®Œæˆç»Ÿè®¡
            eval_duration = time.time() - eval_start_time
            success_count = len(scores) - len(errors)
            logger.info(f"è¯„ä¼°å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{total_metrics}, å¤±è´¥: {len(errors)}/{total_metrics}, æ€»è€—æ—¶: {eval_duration:.2f}ç§’")

            # å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•åœ¨metadataä¸­
            error_msg = None
            if errors:
                error_msg = f"éƒ¨åˆ†æŒ‡æ ‡è¯„ä¼°å¤±è´¥: {errors}"
                logger.warning(error_msg)

            # å‡†å¤‡å“åº”æ–‡æœ¬ï¼ˆå¦‚æœè¶…è¿‡200å­—åˆ™æˆªæ–­ï¼‰
            response_display = eval_sample.response
            response_full_length = len(eval_sample.response)
            if len(response_display) > 200:
                response_display = response_display[:200] + "..."
            
            # å‡†å¤‡å¬å›å†…å®¹ï¼ˆç¼©ç•¥æ˜¾ç¤ºï¼Œæ¯ä¸ªcontextæœ€å¤šæ˜¾ç¤º100å­—ï¼‰
            contexts_display = []
            for i, ctx in enumerate(eval_sample.contexts):
                if ctx and ctx.strip():
                    ctx_preview = ctx[:100] + "..." if len(ctx) > 100 else ctx
                    contexts_display.append(f"ä¸Šä¸‹æ–‡{i+1}: {ctx_preview}")
                else:
                    contexts_display.append(f"ä¸Šä¸‹æ–‡{i+1}: (ç©º)")
            
            # åˆå¹¶metadataï¼Œæ·»åŠ å“åº”å®Œæ•´é•¿åº¦ä¿¡æ¯
            result_metadata = {
                **(eval_sample.metadata or {}),
                "response_full_length": response_full_length,
            }

            return SampleResult(
                scores=scores,
                reasoning=reasoning,
                error=error_msg,
                user_input=eval_sample.user_input,
                response=response_display,  # æˆªæ–­åçš„å“åº”
                reference=eval_sample.reference,
                contexts=contexts_display,  # ç¼©ç•¥åçš„ä¸Šä¸‹æ–‡åˆ—è¡¨
                metadata=result_metadata,
            )

        except Exception as e:
            eval_duration = time.time() - eval_start_time
            logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ (è€—æ—¶: {eval_duration:.2f}ç§’): {e}")
            import traceback
            logger.debug(f"è¯„ä¼°é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
            return SampleResult(error=str(e))
