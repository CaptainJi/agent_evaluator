"""ä¸»è¿è¡Œå™¨ï¼ˆç¼–æ’æ•´ä¸ªæµç¨‹ï¼‰"""

import time
from typing import Any

from agent_evaluator.adapters.base import PlatformAdapter
from agent_evaluator.core.result import EvalReport, SampleResult
from agent_evaluator.core.sample import EvalSample, TestSample
from agent_evaluator.evaluator.executor import EvaluatorExecutor
from agent_evaluator.utils.logger import get_logger

logger = get_logger(__name__)


class EvaluationRunner:
    """è¯„ä¼°è¿è¡Œå™¨ï¼Œç¼–æ’æ•´ä¸ªè¯„ä¼°æµç¨‹"""

    def __init__(
        self,
        adapter: PlatformAdapter,
        evaluator: EvaluatorExecutor,
        stream: bool = False,
    ):
        """
        åˆå§‹åŒ–è¿è¡Œå™¨

        Args:
            adapter: å¹³å°é€‚é…å™¨
            evaluator: è¯„ä¼°æ‰§è¡Œå™¨
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        """
        self.adapter = adapter
        self.evaluator = evaluator
        self.stream = stream

    async def evaluate_sample(self, test_sample: TestSample) -> SampleResult:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬

        Args:
            test_sample: æµ‹è¯•æ ·æœ¬

        Returns:
            SampleResultå¯¹è±¡ï¼ˆåŒ…å«æ€§èƒ½æŒ‡æ ‡ï¼‰
        """
        sample_start_time = time.time()
        try:
            logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            logger.info(f"å¼€å§‹è¯„ä¼°æ ·æœ¬: {test_sample.user_input[:80]}{'...' if len(test_sample.user_input) > 80 else ''}")
            
            # 1. è°ƒç”¨é€‚é…å™¨è·å–å“åº”ï¼ˆæµå¼æˆ–éæµå¼ï¼‰
            adapter_start_time = time.time()
            logger.info(f"ğŸ“¡ è°ƒç”¨é€‚é…å™¨è·å–å“åº”...")
            response = await self.adapter.invoke(
                test_sample.user_input,
                stream=self.stream,
            )
            adapter_duration = time.time() - adapter_start_time
            logger.info(f"âœ… é€‚é…å™¨å“åº”å®Œæˆ (è€—æ—¶: {adapter_duration:.2f}ç§’)")
            logger.info(f"   - å“åº”é•¿åº¦: {len(response.answer)} å­—ç¬¦")
            logger.info(f"   - ä¸Šä¸‹æ–‡æ•°é‡: {len(response.contexts)}")

            # 2. æµå¼å®Œæˆåï¼Œç«‹å³è°ƒç”¨RagaséªŒè¯ï¼ˆç»Ÿä¸€æ—¶æœºï¼‰
            logger.info(f"ğŸ“Š å¼€å§‹Ragasè¯„ä¼°...")
            eval_sample = EvalSample.from_response(test_sample, response)
            result = await self.evaluator.evaluate(eval_sample)

            # 3. å°†æ€§èƒ½æŒ‡æ ‡ä¼ é€’åˆ°ç»“æœä¸­
            result.performance = response.performance

            sample_duration = time.time() - sample_start_time
            if result.is_success:
                logger.info(f"âœ… æ ·æœ¬è¯„ä¼°æˆåŠŸ")
                logger.info(f"   - å¹³å‡åˆ†: {result.average_score:.4f}")
                logger.info(f"   - å¾—åˆ†è¯¦æƒ…: {result.scores}")
                logger.info(f"   - æ€»è€—æ—¶: {sample_duration:.2f}ç§’")
            else:
                logger.warning(f"âŒ æ ·æœ¬è¯„ä¼°å¤±è´¥: {result.error}")
                logger.warning(f"   - æ€»è€—æ—¶: {sample_duration:.2f}ç§’")
            logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

            return result

        except Exception as e:
            sample_duration = time.time() - sample_start_time
            logger.error(f"è¯„ä¼°æ ·æœ¬æ—¶å‘ç”Ÿé”™è¯¯ (è€—æ—¶: {sample_duration:.2f}ç§’): {e}")
            import traceback
            logger.debug(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
            return SampleResult(error=str(e))

    async def evaluate_batch(
        self,
        test_samples: list[TestSample],
    ) -> EvalReport:
        """
        æ‰¹é‡è¯„ä¼°æ ·æœ¬

        Args:
            test_samples: æµ‹è¯•æ ·æœ¬åˆ—è¡¨

        Returns:
            EvalReportå¯¹è±¡
        """
        report = EvalReport()
        report.start_time = time.time()
        total_samples = len(test_samples)

        logger.info(f"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        logger.info(f"â•‘  å¼€å§‹æ‰¹é‡è¯„ä¼°                                                  â•‘")
        logger.info(f"â•‘  æ€»æ ·æœ¬æ•°: {total_samples:<45} â•‘")
        logger.info(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        # ä½¿ç”¨é€‚é…å™¨ä½œä¸ºå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        async with self.adapter:
            for idx, test_sample in enumerate(test_samples, 1):
                logger.info(f"\nğŸ“ æ ·æœ¬è¿›åº¦: [{idx}/{total_samples}]")
                result = await self.evaluate_sample(test_sample)
                report.add_result(result)
                
                # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
                success_count = report.total_samples - report.failed_samples
                logger.info(f"ğŸ“ˆ å½“å‰ç»Ÿè®¡: æˆåŠŸ {success_count}/{idx}, å¤±è´¥ {report.failed_samples}/{idx}")

        report.finalize()
        logger.info(f"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        logger.info(f"â•‘  æ‰¹é‡è¯„ä¼°å®Œæˆ                                                  â•‘")
        logger.info(f"â•‘  æ€»æ ·æœ¬æ•°: {report.total_samples:<45} â•‘")
        logger.info(f"â•‘  æˆåŠŸ: {report.total_samples - report.failed_samples:<48} â•‘")
        logger.info(f"â•‘  å¤±è´¥: {report.failed_samples:<48} â•‘")
        logger.info(f"â•‘  æ€»è€—æ—¶: {report.duration:.2f}ç§’{'':<42} â•‘")
        logger.info(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        return report
