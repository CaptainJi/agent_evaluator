极栈4.1.1标准版部署手册
=======================

# 1.部署方案简介

极栈平台基于k8s云原生部署，平台涉及的前后端应用及中间件均通过容器方式运行，支持单节点部署及后续动态扩缩容，支持离线部署，基于封装ansible自动化运维工具的容器镜像实现快捷高效集群化部署，部署脚本及配置文件通过镜像方式进行版本管理。

# 2.部署前-前期沟通与摸排

极栈5.0标准版部署的环境要求如下，如有国产操作系统适配、ARM架构适配、国产显卡适配以及信创国产化适配需求需要额外定制及配套部署文档。

前期沟通需要主要摸排如下几个方面，以确保现场环境满足部署要求，并决定部署标准版本的可行性以及评估非标适配的改造成本，避免项目延期的风险

| 序号 |                                                    前置要求                                                    | 摸排结果 |
| :--: | :-------------------------------------------------------------------------------------------------------------: | :------: |
|  1  |                                         服务器资源CPU、内存、显卡等配置                                         |          |
|  2  |                                     操作系统要求：建议使用ubuntu2204 server                                     |          |
|  3  | 网络互通要求：集群规划好内网IP，且保障 内网网卡互通（内网IP双向互通，通过ping命令验证），且服务器建议访问互联网 |          |
|  4  |                         网络带宽要求：算力集群使用专用的roce或IB网络交换机，带宽10Gb/s                         |          |
|  5  |    存储性能要求：本地存储系统盘和数据盘建议SSD固态盘，共享存储（建议闪存或接近ssd io效率）可以挂载至服务器上    |          |
|  6  |                 操作系统和网络端口权限要求：服务器的root账号权限 和 现场内网访问服务器所有端口                 |          |
|  7  |                                   远程要求：现场确定VPN远程或者跳板机远程方式                                   |          |
|  8  |              是否有其他额外适配工作：国产GPU适配、中间件国产化适配、国产操作系统适配等非标适配工作              |          |

## 2.1 服务器配置要求

详见资源规格说明书

| 序号 | 服务器类型 |  操作系统  | CPU核心数 | 内存GB | 系统盘 | 硬盘GB |                                           共享存储                                           |                                                    网络                                                    |           节点用途           |
| :--: | :--------: | :---------: | :-------: | :----: | :----: | :----: | :------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------: | :---------------------------: |
|  1  |    CPU    | ubuntu22.04 |    16    |   32   |  100G  | 10000G |                                            非必要                                            | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |       Harbor镜像仓库-主       |
|  2  |    CPU    | ubuntu22.04 |    16    |   32   |  100G  | 10000G |                                            非必要                                            | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |       Harbor镜像仓库-备       |
|  3  |    CPU    | ubuntu22.04 |     8     |   16   |  100G  |  300G  |                                            非必要                                            | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |        K8S的master节点        |
|  4  |    CPU    | ubuntu22.04 |     8     |   16   |  100G  |  300G  |                                            非必要                                            | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |        K8S的master节点        |
|  5  |    CPU    | ubuntu22.04 |     8     |   16   |  100G  |  300G  |                                            非必要                                            | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |        K8S的master节点        |
|  6  |    CPU    | ubuntu22.04 |    64    |  128  |  100G  | 1800G | 1.客户提供高可用的共享存储服务，支持在线扩容，快照备份恢复等功能，至少20TB以上，后续按需扩容 | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |    K8S的woker节点-平台节点    |
|  7  |    CPU    | ubuntu22.04 |    64    |  128  |  100G  | 1800G | 1.客户提供高可用的共享存储服务，支持在线扩容，快照备份恢复等功能，至少20TB以上，后续按需扩容 | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 | K8S的woker节点-平台中间件节点 |
|  8  |    GPU    | ubuntu22.04 |    128    |  256  |  100G  | 3500G | 1.客户提供高可用的共享存储服务，支持在线扩容，快照备份恢复等功能，至少20TB以上，后续按需扩容 | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |  K8S的woker节点-GPU算力节点  |
|  9  |    GPU    | ubuntu22.04 |    128    |  256  |  100G  | 3500G | 1.客户提供高可用的共享存储服务，支持在线扩容，快照备份恢复等功能，至少20TB以上，后续按需扩容 | 1.整个集群内网互通，特别是k8s集群，网卡之间网卡需要互通2.内网交换机千兆及以上规格3.部分功能可能需要访问外网 |  K8S的woker节点-GPU算力节点  |

## 2.2 基础组件要求

| 序号 |    软件名称    |         版本         |
| :--: | :------------: | :------------------: |
|  1  | 操作系统及架构 | ubuntu2204 sever x86 |
|  2  |    显卡驱动    |      nvidia 535      |
|  3  |     Docker     |    20.10.8及以上    |
|  4  |      K8S      |        1.21.5        |
|  5  |     Harbor     |        2.3.3        |

## 2.3 应用组件版本要求

| 序号 |        软件名称        |             版本             |
| :--: | :---------------------: | :--------------------------: |
|  1  |          MySQL          |            8.0.19            |
|  2  |          Redis          |            6.2.0            |
|  3  |          Minio          | RELEASE.2022-05-08T23-50-31Z |
|  4  |       Postgresql       |             11.5             |
|  5  |       Prometheus       |           v2.27.1           |
|  6  |         Gitlab         |    gitlab-ce:14.8.2-ce.0    |
|  7  | Nginx-inress-controller |            0.26.1            |
|  8  |          Nacos          |            v2.3.0            |
|  9  |      Xxl-job-admin      |            2.3.0            |
|  10  |        Rabbitmq        |       3.8.5-management       |
|  11  |          Nginx          |            1.24.0            |

# 3. 部署前准备工作

   本次部署以两个节点离线部署为例展开具体部署步骤说明。
   3.部署前-部署方案确定与准备

## 3.1离线安装包准备

| 序号 |安装包名称|大小|安装包用途|初始化方式|下载路径|
| :--: | :--------: | :----: | :-------------------------------- | :-------------------------------- | :-------------------------------- |
|  1  | ezdown.sh | 约1MB | 操作系统以上的初始化shell脚本用于操作系统以上的初始化、磁盘挂载、驱动安装、离线安装docker、harbor等基础组件，启动ezstack容器进行平台部署，并进行ezstack部署镜像版本管理。需要2,3,4三个文件配合使用 | 1.运维提供离线包或者在线下载地址\n 2.上传到目标环境的deploy-machine节点的/data目录下\n 3.根据项目需要修改脚本中IMAGE_NAME（ 调用的部署镜像）配置\n 4.后续根据教程执行 bash /data/ezdown.sh |172.16.100.156:30901/user-file/ezdown.sh|
|  2  | harbor-install-no-images.tar.gz | 约2GB | Harbor仓库及显卡驱动、docker、操作系统基础组件的离线安装包用于操作系统以上的初始化，配合ezdown.sh脚本，提供初始化安装涉及的离线包 | 1.运维提供离线包或者在线下载地址2.上传到目标环境的deploy-machine节点的/data目录3.根据项目需要修改脚本中IMAGE_NAME(调用的部署镜像)配置 4.后续根据教程为ezdown.sh脚本提供初始化离线包 |172.16.100.156:30901/user-file/harbor-install-no-images.tar.gz|
|  3  | ezstack-4.1.1.tar | 约4GB-5GB | 部署工具docker镜像导出tar包上传到服务器以后执行docker load -i ezstack-5.0.tar 导入到服务器，镜像里封装了操作系统基础rpm包、docker离线包、k8s离线安装包、ansible自动化运维工具，以及平台部署配置和安装脚本。 | 1.运维提供离线包或者在线下载地址，以及md5值2.上传到deploy-machine节点/data目录下待安装好docker以后执行docker load导入到deploy-machine后即可3.后续配合ezdown.sh启动ezstack容器，分步骤自动化的进行整个平台的集群化离线部署 |172.16.100.156:30901/user-file/ezstack-4.1.1.tar|
|  4  | harbor-data-4.1.1.tar | 200GB-300GB | Harbor镜像仓库全量镜像离线还原包（克隆测试环境Harbor）平台离线部署涉及到k8s各组件镜像、平台前后端组件镜像、以及平台业务实例启动依赖的基础算法镜像等 | 1.运维提供离线包或者在线下载地址，以及md5值2.上传到deploy-machine节点/data目录下并执行 `tar cvf harbor-data-5.0.tar`解压，目录不可变更3.和harbor-install-no-images.tar.gz包一起配合ezdown.sh脚本一键离线还原所有镜像 |172.16.100.156:30901/user-file/harbor-data-4.1.1.tar|

## 3.2服务器配置确认：

详细配置根据实际情况填写：                                                                                                                                                                                                                            |

| 序号 |   服务器类型   |   IP地址   |  操作系统  | CPU核心 | 内存GB | 系统盘 | 数据盘GB |   显卡类型   |             共享存储             |                   节点用途                   |
| :--: | :-------------: | :---------: | :---------: | :-----: | :----: | :----: | :------: | :----------: | :-------------------------------: | :-------------------------------------------: |
|  1  | 云主机CPU服务器 | xx.xx.xx.xx | ubuntu22.04 |   32   |  128  |  100G  |  6000G  |      无      |              非必要              |               Harbor镜像仓库-主               |
|  2  | 云主机CPU服务器 | xx.xx.xx.xx | ubuntu22.04 |   32   |  128  |  100G  |  6000G  |      无      |              非必要              |               Harbor镜像仓库-备               |
|  3  | 云主机CPU服务器 | xx.xx.xx.xx | ubuntu22.04 |   128   |  512  | 1000G |  2000G  |              | 通过nfs协议提供共享存储给集群使用 |    K8S master节点&K8S的woker节点-平台节点    |
|  4  | 云主机CPU服务器 | xx.xx.xx.xx | ubuntu22.04 |   128   |  256  | 1000G |  7000G  |              | 通过nfs协议提供共享存储给集群使用 | K8S master节点&K8S的woker节点-平台中间件节点 |
|  5  | 云主机CPU服务器 | xx.xx.xx.xx | ubuntu22.04 |   128   |  256  | 1000G |  1800G  |              | 通过nfs协议提供共享存储给集群使用 | K8S master节点&K8S的worker节点-平台中间件节点 |
|  6  | 裸金属GPU服务器 | xx.xx.xx.xx | ubuntu22.04 |   112   |  128  | 1000G |  5500G  | 英伟达T4 * 2 | 通过nfs协议提供共享存储给集群使用 |            K8Sworker节点算力服务器            |
|  7  | 裸金属GPU服务器 | xx.xx.xx.xx | ubuntu22.04 |   192   |  2048  | 1000G |  7000G  | 英伟达H100*8 | 通过nfs协议提供共享存储给集群使用 |            K8Sworker节点算力服务器            |

## 3.3部署流程说明

部署流程图如下：

## 3.4部署脚本、镜像等工具说明

```bash
bash /data/ezdown.sh     #不输入任何参数，则打印工具帮助信息

Usage: ezdown [options] [args]
  option:
    -B         before start deploy-container
    -c         print common cmd
    -D         install docker
    -H         install harbor
    -I         init-deploy-machine: set hostname,time;check /data size
    -K         upgrade linux kernel (to do only deploy-machine centos7 linux kernel needs > 3.X)
    -M         archive project to  images (to do only deploy-machine)
    -N         install nvidia driver(to do only deploy-machine)
    -P         install docker-compose
    -S         start kubeasz in a container  as ezstack
    -T         sshpass copy ssh token  ~/.ssh/id_rsa.pub
    -V         mount disk to /data
```

接下来，从第4章节开始部署实践吧

# 4.部署中-deploy-machine节点服务器初始化

deploy-machine节点：作为集群的第一个节点，建议选择harbor服务器节点作为deploy-machine节点，该节点的初始化需要ezdown.sh脚本及离线包分步骤的执行，最终在该节点上启动ezstack容器，利用容器内部的ansible工具进行集群其他节点的初始化，以及后续的k8s集群安装，平台安装。

离线包下载路径:

```bash 
wget 172.16.100.156:30901/user-file/ezdown.sh # 下载 ezdown.sh
wget 172.16.100.156:30901/user-file/harbor-install-no-images.tar.gz # 下载 harbor-install-no-images.tar.gz
wget 172.16.100.156:30901/user-file/ezstack-4.1.1.tar #下载 ezstack-4.1.1.tar
wget 172.16.100.156:30901/user-file/harbor-data-4.1.1.tar #下载 harbor-data-4.1.1.tar
```

## 4.1上传离线包至deploy-machine节点指定路径

提前把数据盘（至少2TB）挂载至deploy-machine节点的/data目录下
参考命令：

```bash
fdisk -l
mkfs.ext4 ${disk}
mkdir /data
uuid=$(blkid "${disk}" | awk '{print $2}' | sed 's/"//g')
echo "${uuid} /data ext4 defaults 0 0" >> /etc/fstab
mount -a

通过sftp工具上传离线包 ezdown.sh、harbor-install-no-images.tar.gz、ezstack-4.1.1.tar、harbor-data-4.1.1.tar至deploy-machine节点的/data目录下，并进行解压
#解压harbor-install-no-images.tar.gz、harbor-data-4.1.1.tar离线包，ezstack-4.1.1.tar是docker save 出来的镜像tar包，后续安装好docker以后，执行docker load -i harbor-data-4.1.1.tar命令加载
cd /data
tar zxvf harbor-install-no-images.tar.gz
tar xvf  harbor-data-4.1.1.tar
```

## 4.2deploy-machine节点时区、主机名、驱动、内核升级等基础组件初始化

```bash
#ezdown脚本实现集群的服务器初始化（sshpass、nfs等系统包）、数据盘挂载、升级内核（大于3.X）、安装docker、安装nvidia-smi驱动（如需）、免密登录

#挂载数据盘至/data，高危操作需要交互式确认，如已具备则忽略，该操作会格式化数据盘数据，请提前确认好数据盘可清空， deploy-machine已经在4.1章节手动提前挂载到/data目录，该步骤无需操作
#bash /data/ezdown.sh -V

#设置主机名和时区，安装基础deb和rpm包，如nfs\ntp\sshpass等，自动检测有网在线安装，离线在安装内置离线包
bash /data/ezdown.sh -I

#升级centos7系列的操作系统内核，其他类型的操作系统可以忽略
#bash /data/ezdown.sh -K #升级centos 7.x默认内核3.10至5.4.90, 如已具备则忽略

#如果是单节点部署，harbor又是GPU节点，则执行安装英伟达显卡驱动，默认安装535驱动，如果是centos类的操作系统，执行-N后，手动重启服务器，再次执行-N才可以安装驱动
bash /data/ezdown.sh -N
```

## 4.3deploy-machine节点与集群所有节点免密ssh登录

```bash
#配置服务器集群免密登录，如已免密登录则忽略
#编辑配置文件，参照参考格式配置集群各个节点的内网ip、用户名、密码和端口
#172.16.1.x:root:Password:22
vi /data/harbor-install/scripts/copy-ssh-key.list
#设置集群免密登录
bash /data/ezdown.sh -T
```

## 4.4安装harbor并自动还原测试环境全量镜像

```bash
bash /data/ezdown.sh -D #安装docker 20.10.5以上版本，高危操作需要交互式确认，如已具备则忽略
bash /data/ezdown.sh -P #安装docker-compose,如已具备则忽略
bash /data/ezdown.sh -H #安装harbor,基于/data/harbor-data已有的镜像数据，还原到新的环境，无需单独逐个拉取镜像，业务强关联
#手动登录harbor地址，xx:8099
```

### 4.5启动ezstack部署容器

```bash
# docker load -i下ezstack镜像

docker load -i /data/ezstack-4.1.1.tar

# copy目的，方便宿主机直接查看容器配置文件

bash /data/ezdown.sh -B #把镜像里面的部署脚本和离线包等copy 到deploy-machine服务器的/data/src/evstack-install/目录下

bash /data/ezdown.sh -S #启动ezstack容器，后续在下一章节配置好配置文件后，调用ezstack容器内部ansible，调用对应playbook去逐步部署分布式的极栈平台
bash /data/harbor-install/create_project_harbor.sh   #自动创建sdk等harbor项目空间
```

## 4.6创建并配置平台部署所需要的配置文件

```bash
# 不可以在脚本调用，手动在终端执行，dk的别名配置手动生效

source  /root/.bashrc

# 生成平台安装的配置文件，evstack-4.1.1为业务集群名，可自定义，后面的操作需要和这个保持一致

dk /data/src/evstack-install/ezctl new evstack-4.1.1

关于配置文件的修改说明，config-k8s.yaml和platform_hosts必须配置正确，否则k8s部署和平台部署都无法进行：
#编辑集群配置文件，/data/src/evstack-install/clusters/evstack-4.1.1路径下的config-k8s.yaml和platform_hosts

cd /data/src/evstack-install/clusters/evstack-4.1.1
#编辑k8s集群部署的配置文件-config-k8s.yaml
vi config-k8s.yaml
#编辑极栈平台离线部署配置文件-platform_hosts ，节点标签如何分类可以参考4.7章节
vi platform_hosts
```

## 4.7platform_hosts关于worker组和worker-gpu组的节点成员的标签node_label说明

如果是单节点部署（cpu+gpu），则k8s节点打上所有节点label

```bash
kubectl label node node-xxx  cpu=true middleware=true  minio=true  apps=true  evaluation-data=1 gpu=true machine=gpu nvidia-type=1 ov-type=1 pack-debug-type=1 pack-type=1 preprocess-type=1 sdk-algo-type=1 tensorboard=1 train-type=1
```

如果是多节点部署，k8s节点标签分类如下：

|    节点角色    |            节点用途            | GPU要求 |                                                                              节点标签                                                                              |                                                                                                                                                                                             备注                                                                                                                                                                                             |
| :------------: | :----------------------------: | :-----: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| cpu-middleware |    部署所有中间件及基础组件    | 非必须 |                                                                      cpu=true middleware=true                                                                      |                                               1.cpu=true : 用来部署node-exporter等基础组件2.middleware=true：除了minio以外的所有中间件如果资源充足可以设置两节点以上（mysql gitlab等持久化数据都是nfs提高的pvc）支持中间件pod漂移,包括prom\ingress\redis\mq\mysql\gitlab\xxl-job\nacos等3.支持多节点高可用、可与其他任意节点合并复用，支持迁移                                               |
| cpu-nfs&minio | 部署minio对象存储和nfs网络存储 | 非必须 |                                                                         cpu=trueminio=true                                                                         | 1.cpu=true : 用来部署node-exporter等基础组件2.minio=true：用来部署minio对象存储，对存储性能要求较高，建议2TB以上，支持按需扩容3.minio节点必须部署nfs-server和ftp，用来nfs协议和ftp协议共享minio桶，以及为apps多节点提高logs共享存储和算力多节点提高tensorboard的数据4.存储层面高可用&节点无需高可用、可与其他任意节点合并复用，不支持在线迁移（如涉及数据盘迁移和内网ip变动需要产品运维配合） |
|    cpu-apps    |    部署前后端应用及基础组件    | 非必须 |                                                                         cpu=true apps=true                                                                         |                                                                                                                        1.cpu=true : 用来部署node-exporter等基础组件2.apps=true：用来部署前后端服务和nginx等3.支持多节点高可用、或者可与其他任意节点合并复用，支持迁移                                                                                                                        |
|  gpu-allinone  |            算力节点            |  必须  | cpu=true gpu=true evaluation-data=1 machine=gpu nvidia-type=1 ov-type=1 pack-debug-type=1 pack-type=1 preprocess-type=1 sdk-algo-type=1 tensorboard=1 train-type=1 |                                         1.cpu=true : 用来部署node-exporter等基础组件2.gpu=true：用来gpu-exporter和csi/fuse等基础组件（只有算力节点需要），3.其他标签：包括训练、测试、推理、封装等任务必须标签，如果是多节点可以继续拆分不同任务都多节点，数据盘至少1TB（docker镜像）4.支持多节点扩容，支持拆分或者可与其他任意节点合并复用，支持迁移                                         |

harbor和cvat服务是基于docker-compose部署，建议单独的节点进行部署

|  节点角色  |      节点用途      | GPU要求 |       节点标签       |                                                                                                                       备注                                                                                                                       |
| :--------: | :----------------: | :-----: | :-------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| cpu-harbor | 部署harbor镜像仓库 | 非必须 | 非k8s部署，无需打标签 | 1.非k8s部署，但是可以和其他任意节点复用2.对存储性能要求较高，建议2TB以上，支持按需扩容，存储高可用3.支持多节点高可用（多harbor相互同步）或者可与其他任意节点合并复用（单独存储），不支持在线迁移（如涉及数据盘迁移和内网ip变动需要产品运维配合） |
|  cpu-cvat  |  部署标注服务组件  | 非必须 | 非k8s部署，无需打标签 |                                                                     4.非k8s部署，但是可以和其他任意节点复用，不支持在线迁移（如涉及数据盘迁移和内网ip变动需要产品运维配合）                                                                     |

# 5.部署中-平台集群初始化及平台部署激活并开启定时备份

## 5.1平台部署-deploy-machine节点之外的集群其他节点初始化

```bash
###如果是单节点部署平台，如下操作已经通过ezdown.sh脚本实现，无需再执行
#重启下ezstack容器，保障上面修改的配置文件同步进容器内部（宿主机和容器内部使用的同一份配置，需要重启刷新）
docker restart ezstack

#除了deploy-machine以外其他节点的驱动安装\内核升级\数据盘挂载 因涉及到服务器重启和误操作带来的数据丢失风险，需要单独执行，执行过程中需要交互式输入确认执行指令，如已满足条件，也可以跳过执行，脚本会自动检测是否满足条件，满足条件的会跳过执行

#逐一升级集群中自动检测到centos 7.x 操作系统的内核为3.X的内核至5.4.90，不符合升级条件的会自动跳过
dk /data/src/evstack-install/ezctl upgrade-kernel  evstack-4.1.1

#逐一挂载集群中/data没有挂载的服务器，挂载数据盘，如果已经挂载可忽略， 会交互式输入目标磁盘以及二次确认是否需要格式
dk /data/src/evstack-install/ezctl disk-mount  evstack-4.1.1

#数据盘挂载好一会，把ezdown.sh和harbor-install-no-images.tar.gz分发到集群其他节点
dk /data/src/evstack-install/ezctl copy-ezdown-cluster   evstack-4.1.1
##需要更新ezctl和100.playbook

#逐一安装集群worker-gpu下显卡驱动，默认是nvidia 535，如已经安装则跳过，安装驱动前会
dk /data/src/evstack-install/ezctl install-gpu-driver  evstack-4.1.1
#如目标GPU服务器 lsmod | grep nouveau 如果仍然返回数据，说明服务器nouveau驱动仍在加载，必须重启服务器才能生效，需要手动重启服务器后，再次执行install-gpu-driver，如仍然报错可以登录目标服务器手动执行bash /data/ezdown.sh -N
dk /data/src/evstack-install/ezctl install-gpu-driver  evstack-4.1.1

#自动集群所有节点的os-base基础包，比如ntp nfs socat openssl等基础包，针对非常见的操作系统需要运维提前搞定适配基础rpm或deb包，等同于在每个节点执行 bash /data/ezdown.sh -I
dk /data/src/evstack-install/ezctl  install-base-os  evstack-4.1.1

#注意：如果分布式初始化其他节点的任务执行报错，把ezdown.sh和harbor-install-no-images.tar.gz复制到每个节点/data目录下，然后手动执行ezdown.sh进行初始化，参见4.2
```

```bash
#01步骤：针对整个集群关闭防火墙、自动设置主机名、开启ntp时间同步、开启nfs-server、安装docker并配置daemon.json、分发ezdown.sh及harbor-install-no-images.tar.gz至各个节点
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 01
```

## 5.2平台部署-集群环境巡检

```bash
#自动巡检cpu节点docker、数据盘、nfs、ntp、daemon-json、gpu-docker开启2375、主机名、时区、时间同步、登录自建harbor，用来评估集群是否满足部署要求和业务运行要求。如不满足，可能会导致部署失败或者平台业务无法正常运行
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 02
```

## 5.3平台部署-k8s集群安装

```bash
##安装k8s以后，如果calico或者coredns的pod无法正常运行，可以先重启下服务器试试，因为前面服务器操作系统层面初始化的操作较多，可能会一影响服务路由的创建或加载
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 03

tail -f /data/src/evstack-install/install-k8s-result.log
```

## 5.4平台部署-k8s节点分类打标签

```bash
docker restart ezstack #重启部署容器，确保/etc/hosts 在宿主机和部署容器内部同步
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 04
```

## 5.5平台部署-基于k8s的平台基础组件安装

```bash
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 05
```

## 5.6平台部署-基于k8s的平台中间件应用安装及数据初始化

```bash
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 06
```

## 5.7平台部署-基于k8s的平台前后端应用安装

```bash
5.7平台部署-基于k8s的平台前后端应用安装
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 07
```

##注意：07步骤最后的部分是安装图像标注cvat和文本语音标注label-studio以及两个智能标注的算法，且两个算法是基于nuclio severless架构，通过docker-compose部署，部分现场环境docker-compose节点和k8s的节点是同一个，可能会导致docker-compose服务启动创建iptables网络规则失败，可以重启服务器后再次重试07,或者手动到规划好的cvat-server节点执行
#启动图像标注服务cvat
mkdir -p /data/cvat/volumes/{cvat_data,cvat_logs,cvat_prev,cvat_share,cvat_temp}
chown -R 1000:1000 /data/cvat/volumes/{cvat_data,cvat_logs,cvat_prev,cvat_share,cvat_temp}

/usr/local/bin/docker-compose -f /data/cvat/docker-compose.yml  up -d

/usr/local/bin/docker-compose -f /data/label-studio/docker-compose.yml  up -d

bash /data/cvat-nuclio/serverless/deploy_algo.sh /data/cvat-nuclio/serverless/onnx/WongKinYiu/yolov7/nuclio

bash /data/cvat-nuclio/serverless/deploy_algo.sh /data/cvat-nuclio/serverless/pytorch/facebookresearch/sam/nuclio

#检查结果
nuctl get functions --platform local
#nuctl delete functions  <函数名>  --platform local

```
## 5.8平台部署-初始化mysql及minio初始数据及参数配置

初始化minio和mysql数据
```bash
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 08
```

获取nacos配置需要的参数gitlab初始密码和k8s api token

```bash
#获取gitlab初始密码和k8s api token ，每个环境的密码和token都不一样，下面是案例，请不要照抄
cat /data/src/evstack-install/conf/08-config-args/nacos_args.txt

1. 通过User: root / Password: kxcnayVLuvBteRKI/xGZQSpfUFcHSNmh4ZY7Jw+O9jM=  手动登录gitlab http://172.16.100.:31080 修改密码为建议密码,务必修改密码，初始密码有效期只有1天，过期失效，并创建gitlab token ，并配置在nacos里面指定命名空间下ev-stack-core.yaml
2. 手动复制 k8s的 admin token, 并配置在nacos里面指定命名空间下ev-stack-core.yaml:  token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IldjWmh2aGV5MVZBMXVWb0duNXJ4TlNzanZzT1hFYzQzMy1vS0U5eHBGa1UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJldnN0YWNrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLTQ4YnA4Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMDU4ZjFlNmEtYWM0YS00MDYzLWE1YWMtMzUzMzA4MWE5OGVlIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmV2c3RhY2s6YWRtaW4ifQ.edMVV0S1jPYePq85WJt-rHR1Z33jYqabeE04Z_GvctBDYxgpyCde77B3rr6gvdzqD2hhcRtz9J9HvHs1ZWYXVNJ6Z48x9DNnz4Z1-yZgihgW-3MsjF1uuGn10vSx3kqA0vuhQ2OSQpNDIHt1mYAyuz0BoPLx3XDnr62F_1WhBIYT0stRvB_9YDbpQR4zEQBYJ5_YbvKUypDi5AjJiT31eOl35oBYfeRX-axaJKUMHFVQGp9usxwX6z8VPEs_-RoAxCXphR0iRPr3k00IMuhkaZYEpeM1nnoAe3izV3ZHEHKY964bEqgLBelkp0dswpzRryb2TacC1254A28WJKJ8wA
3. 配置其他参数如harbor-ip等
```

修改gitlab初始化密码和生成token
浏览器登录masterip:31080 登录gitlab, 用户名root，密码是/data/src/evstack-install/conf/08-config-args/nacos_args.txt里面的

登录后，修改密码，默认修改Y36(py5jz

修改密码后重新登录，然后创建token， 复制创建好的gitlab token， 后续配置在nacos里

配置nacos
浏览器登录masterip:31023/nacos  ，用户名密码nacos/nacos
选择配置管理-配置列表-命名空间选择test ，分别编辑ev-stack-core.yaml 和bussiness.properties

主要修改的内容：

```bash
1.ev-stack-core.yaml的配置
#k8s的token 以及coding_uri
  mixture:
    kubernetes:
      ucloud:
        name: ucloud
        base_uri: https://kubernetes.default.svc.cluster.local:443
        call_back_uri: http://gateway-svc:1100
        ###coding_uri必须为ip:port，nodeport，且ip为浏览器登录平台的ip(如果内外网ip一致，则填服务器网卡ip)
        coding_uri: http://172.16.100.133:30002
        namespace: evstack
        ###token需要替换当前k8s的token，参见/data/src/evstack-install/conf/08-config-args/nacos_args.txt
        token: eyJhbGciOiJSUzI1NiIsImtpZCI6Ik5lNkpTUGN3Rk16UlFBZlpPdktPNlZqekFEOVZPNUxwRGc5b3ljNy00bUEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJldnN0YWNrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLXdnanZ3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMjY4N2UwMGEtYmNjZS00ZjY5LWI1NjctMWRhMGNjYjk2ZjU4Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmV2c3RhY2s6YWRtaW4ifQ.gOgb6LbWsOhgKqCjHQMtme-uWJh7JpAAodXEE-t0z7CRwyud1DaWCWfjvLAvVGVwFQFNSnSsmmkyWbt76HDyo8NTTPNWHftSc38Z86VVgFSPi3fMOn9yTOD2Zs30JzZQLcajTRAJSllYrOS287KiGBEifn342Sss4l_J7DwSnYHD0oEnqUEmW1899KjUhWDiZjJpzHGJDVkWn46VnKylpOxtLeWflTA7oHaHiV3fTly2zH4RcTntHaKa6hkruR34eOLBtjkmwDxJWTgOhjOm_4wYGIObbW2qcrRxwVxvEtM7m7SEPCvvCLdqncPt92BI08s0WhQAofqgaAYmCMM5Eg

 #gitlab的token
    gitlab:
      ucloud:
        name: ucloud
        ip: gitlab-svc
        port: 31080
        ###gitlab的初始化密码在/data/src/evstack-install/conf/08-config-args/nacos_args.txt获取后，然后必须修改密码，然后创建token，填入到这里
        ###如果重装gitlab则需要重新配置，并删除auth数据库的t_gitlab_user表所有数据
        token: XC_ZNQifXyxj_F_xQpWG
 #minio相关的配置url 、cvaturl这两个
    minio:
      ucloud:
        name: ucloud
        ###公网地址-后端返给前端的minio地址，前端在页面直接访问上传下载数据集和logo背景图，该ip必须是客户所在网络的ip和端口（浏览器访问的地址），如果客户浏览器和服务器在同一个网络，则可以和privateurl一致
        #url: http://172.16.100.134:30900  ##不建议
        ##建议，无论内外网，建议改成平台对应的ip+端口进行反向代理
        url: http://172.16.100.133:30002/minio
    ##内网地址-后端应用pod访问minio走svc和nodeport都可以,但是经过多次验证，如果客户存在公网IP和内网IP，此处的minio必须是minio-svc
        #privateUrl: http://10.30.10.134:30900 ##不建议，客户标准内外网隔离的情况下，上传数据文件会报错
        privateUrl: http://minio-svc:9000
        ##内网地址-标注服务（不是k8s部署，所以不能用svc通信）也通过该地址和minio通信，所以必须使用服务器内网IP和nodeport
        cvaturl: http://10.30.10.134:30900

2.bussiness.properties的主要配置
替换当前环境使用的harbor地址，以及极星平台部署的授权中心地址（如有）
back.system.harbor.url=172.16.100.136:8099
node.auth.url=http://172.16.100.137:5001
infer.system.harbor.url=172.16.100.149:8099
```

## 5.9重启nacos及前后端服务

```bash
#修改了nacos配置以后，需要重启后端服务使配置生效
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 09
```bash
## 5.10开启定时备份任务
```bash
#定期修改k8s集群证书、定期备份etcd数据、定期备份mysql数据、定期清理xxl-job-log历史数据等
dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 10
```

## 5.11登录平台获取设备摘要并申请激活平台

浏览器登录masterip:30002

# 6.部署后-整理环境信息和账号密码

1.网络端口开通
8099-harbor服务 30002-平台登录 31023-nacos服务 31080-gitlab 30780-xxl-job-admin 30775-prometheus 30901-minio登录 31672-rabbitmq
2.账号密码归档
环境服务器环境信息和各个中间件登录信息和用户名密码统一更新进文档
https://doc.weixin.qq.com/sheet/e3_AegAqgYKAC8CNLpc6WIaTTKyo5CbW?scode=AH8AbAfiAAgsNsSFylAegAqgYKAC8&tab=BB08J2
https://doc.weixin.qq.com/sheet/e3_AegAqgYKAC8CNLe1dvCMuSOi2hLYj?scode=AH8AbAfiAAgkYURSZ0AegAqgYKAC8&tab=k22zn0
3.备份输出连接
项目功能完全交付后，备份现场的mysql和minio等初始化数据，以及维护增量的镜像列表

# 7.部署后-增量定制部署

漏洞修复：

```bash
1.关闭除了平台-30002和minio-30090之外的所以nodeport端口
kubectl delete -f /data/src/evstack-install/conf/conf_example/11-one-key-nodeport-example/yaml
```

# 8.常见问题

## 8.1 rpm包或deb包离线安装报错

针对安装 检查集群环境之前（dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 02）之前的涉及离线安装docker、ntp、nfs等因为操作系统版本问题导致离线包不兼容，导致离线包安装报错的解决方案
1.离线包目前只支持ubuntu2204 server 和 centos7.6 server 离线包
2.如果现场服务器可以连外网

```bash
##针对ubuntu服务器
apt install nfs-kernel-server nfs-common socat conntrack ebtables ipset sshpass ipvsadm

##针对centos服务器
yum install  -y   nvidia-docker2  #需配置yum源
yum install  -y   pciutils net-tools bind-utils telnet  wget curl  vim yum-utils     device-mapper-persistent-data lvm2   nfs-utils ntp ntpdate socat conntrack ebtables  ipset bash-completion sshpass scl-utils openssl ipvsadm
```

3.如何现场服务器不可以连外网

```bash
##针对ubuntu 服务器，如下是必须安装的， 另外docker或者nvidia-docker2的离线包可以参照如下导出
#在有网的服务器下载离线包及其依赖包，在/mnt目录下找到打包
apt-get install --download-only nfs-kernel-server nfs-common -o Dir::Cache::archives="/mnt"
#在无网的现场服务器离线安装
dpkg -i *deb

##针对centos 服务器，如下是必须安装的， 另外docker或者nvidia-docker2的离线包可以参照如下导出
#在有网的服务器下载离线包及其依赖包，在/mnt目录下找到打包
#nvidia-docker2 ，显卡主机必备，GPU节点可选 （安装nvidia-docker2默认会直接安装docker，无需单独安装docker）
##下载离线包：修改yum源 /etc/yum.repos.d/nvidia-docker.repo，
yum install  -y  --downloadonly --downloaddir=./   nvidia-docker2
yum install  -y  --downloadonly --downloaddir=./   pciutils net-tools bind-utils telnet  wget curl  vim yum-utils     device-mapper-persistent-data lvm2   nfs-utils ntp ntpdate socat conntrack ebtables  ipset bash-completion sshpass scl-utils openssl ipvsadm

## GPU服务器安装驱动的时候还有有gcc和make

或者
yum install -y yum-utils pciutils net-tools bind-utils telnet  wget curl  vim yum-utils     device-mapper-persistent-data lvm2   nfs-utils ntp ntpdate socat conntrack ebtables  ipset bash-completion sshpass scl-utils openssl ipvsadm

#麒麟v10
yumdownloader  pciutils net-tools bind-utils telnet  wget curl  vim      device-mapper-persistent-data lvm2   nfs-utils ntp ntpdate socat conntrack ebtables  ipset bash-completion sshpass scl-utils openssl ipvsadm

## GPU服务器安装驱动的时候还有有gcc和make

#在无网的现场服务器离线安装
rpm -ivh --force --nodeps /data/harbor-install/packages/kyv10/nvidia-container-toolkit/*.rpm
#强制安装，但是可能有报错会被忽略

#比较安全，但是有其他依赖yum localinstall -y *rpm

#国产化适配
目前已适配麒麟V10 SP3 20221125版本，内核4.19.90-52.15.v2207.ky10.x86_64 ， 该离线包部署的麒麟V10 SP1的有报错
```

8.2 安装k8s常见报错
针对执行 dk /data/src/evstack-install/ezctl setup  evstack-4.1.1 03的相关报错，可能存在如下问题

```bash
1.支持ubuntu2204和centos7 混合集群

2.如果自动化执行报错，则手动执行安装/data/src/evstack-install目录下

#离线安装集群
 #./bin/kk create cluster -f ./clusters/evstack-4.1.1/config-k8s.yaml -a ./packages/k8s1.21.5-ubuntu2204-amd64.tar.gz
 #删除集群-只清理k8s的组件，docker和镜像不会被清理
 #./bin/kk delete  cluster -f ./clusters/evstack-4.1.1/config-k8s.yaml
 #删除每个节点的 rm -rf /root/.kube/

3.如果ubuntu2204和centos7 混合集群 安装k8s的时候centos节点的pod会启动报错
###如果centos节点基础pod无法正常使用，kubectl describe pod -n kube-system  nodelocaldns-4ll7g 报错：
Failed to create pod sandbox:  open /run/systemd/resolve/resolv.conf: no such file or directory

 解决方案：centos节点创建文件
 mkdir -p  /run/systemd/resolve/
 vi  /run/systemd/resolve/resolv.conf

nameserver 114.114.114.114
search openstacklocal
```

# 9.待优化清单

1.01-playbook优化场景，针对独立的master节点（非worker节点）也要copy damon.json 和 docker.service , 且顺序为control-plane -> worker -> worker-gpu    --已完成

```bash
    - name: copy daemon.json
      copy:
        src:  "{{ base_dir }}/conf/01-init-node/daemon.json"
        dest: /etc/docker/daemon.json
        mode: "0755"
      ignore_errors: true
      when:
        - "'deploy-machine' not in group_names"
        - "'control-plane' in group_names"

    - name: copy daemon.json
      copy:
        src:  "{{ base_dir }}/conf/01-init-node/daemon.json"
        dest: /etc/docker/daemon.json
        mode: "0755"
      ignore_errors: true
      when:
        - "'deploy-machine' not in group_names"
        - "'worker' in group_names"
    - name: copy daemon.json
      copy:
        src:  "{{ base_dir }}/conf/01-init-node/gpu-daemon.json"
        dest: /etc/docker/daemon.json
        mode: "0755"
      ignore_errors: true
      when:
        - "'deploy-machine' not in group_names"
        - "'worker-gpu' in group_names"

    - name: copy docker.service.json only on master machine
      copy:
        src:  "{{ base_dir }}/conf/01-init-node/gpu-docker.service"
        dest: /etc/systemd/system/docker.service
        mode: "0755"
      ignore_errors: true
      when:
        - "'deploy-machine' not in group_names"
        - "'control-plane' in group_names"

    - name: copy docker.service.json only on CPU machine
      copy:
        src:  "{{ base_dir }}/conf/01-init-node/gpu-docker.service"
        dest: /etc/systemd/system/docker.service
        mode: "0755"
      ignore_errors: true
      when:
        - "'deploy-machine' not in group_names"
        - "'worker' in group_names"

    - name: copy docker.service.json only on GPU machine -set up 2375
      copy:
        src:  "{{ base_dir }}/conf/01-init-node/gpu-docker.service"
        dest: /etc/systemd/system/docker.service
        mode: "0755"
      ignore_errors: true
      when:
        - "'deploy-machine' not in group_names"
        - "'worker-gpu' in group_names"
```

2.更新trains服务为最新版本的镜像（ev_stack_trains_new:1.0-SNAPSHOT.tar），解决标准版版环境低代码实例运行卡住的问题 ，后续部署好标准版增量更新
3.修复nginx漏洞修复，发现默认页index.html

```bash
nginx的configmap里加上
server_tokens off; ##隐藏nginx版本信息

##访问nginx自带的默认页的时候直接跳转到404界面
location = /index.html {
    return 404;
}
```

4.新增定时任务-回收资源

```bash
*/30  *  * * *   kubectl get pods | awk '/Evicted/{print $1}' | xargs kubectl delete pod --force
*/60  *  * * *   kubectl get pods | awk '/OutOfgithub/{print $1}' | xargs kubectl delete pod --force
*/30  *  * * *  kubectl get pods | awk '/Terminating/{print $1}' | xargs kubectl delete pod --force
*/30  *  * * *   systemctl restart kubelet
1     1  * * *  docker system prune -f
```
