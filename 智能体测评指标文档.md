# 智能体测评指标文档

## 概述

智能体测评体系是评估AI智能体性能的完整框架，涵盖多个维度的指标体系。本文档基于Ragas项目，详细介绍了智能体测评的四个主要维度：RAG相关测评指标、工具调用及MCP相关指标、通用指标和性能指标。

## 1. RAG相关测评指标

RAG（Retrieval-Augmented Generation）相关指标主要用于评估智能体在检索增强生成任务中的表现。本章节包含9个核心指标，涵盖检索质量、生成质量和端到端评估。

### 1.1 上下文精确度（Context Precision）

**指标定义**：评估检索到的上下文对于回答问题的有用性和精确度。衡量相关文档块在检索结果中的排名位置，分数越高表示相关文档排名越靠前。

**评分范围**：0.0-1.0，分数越高表示上下文精确度越好

**应用场景**：评估智能体检索系统的上下文质量，适用于问答系统、文档检索等场景

**Ragas实现方式**：

#### 基于LLM的上下文精度

| 实现方法             | Ragas类名                               | 评估依据                                           | 适用范围 |
| -------------------- | --------------------------------------- | -------------------------------------------------- | -------- |
| **无参考答案** | `LLMContextPrecisionWithoutReference` | 使用LLM比较 `retrieved_contexts`和 `response`  | 单轮对话 |
| **有参考答案** | `LLMContextPrecisionWithReference`    | 使用LLM比较 `retrieved_contexts`和 `reference` | 单轮对话 |

#### 非基于LLM的上下文精度

| 实现方法                 | Ragas类名                               | 评估依据                                                                | 适用范围 |
| ------------------------ | --------------------------------------- | ----------------------------------------------------------------------- | -------- |
| **基于参考上下文** | `NonLLMContextPrecisionWithReference` | 使用非LLM相似性度量比较 `retrieved_contexts`和 `reference_contexts` | 单轮对话 |

**使用示例（LLM - 无参考答案）**：

```python
from ragas.metrics import LLMContextPrecisionWithoutReference
from ragas import SingleTurnSample

scorer = LLMContextPrecisionWithoutReference(llm=evaluator_llm)
sample = SingleTurnSample(
    user_input="埃菲尔铁塔在哪里？",
    response="埃菲尔铁塔位于巴黎。",
    retrieved_contexts=["埃菲尔铁塔位于巴黎。"]
)
score = await scorer.single_turn_ascore(sample)
```

**使用示例（LLM - 有参考答案）**：

```python
from ragas.metrics import LLMContextPrecisionWithReference
from ragas import SingleTurnSample

scorer = LLMContextPrecisionWithReference(llm=evaluator_llm)
sample = SingleTurnSample(
    user_input="埃菲尔铁塔在哪里？",
    reference="埃菲尔铁塔位于巴黎。",
    retrieved_contexts=["埃菲尔铁塔位于巴黎。"]
)
score = await scorer.single_turn_ascore(sample)
```

**使用示例（非LLM - 基于参考上下文）**：

```python
from ragas.metrics import NonLLMContextPrecisionWithReference
from ragas import SingleTurnSample

scorer = NonLLMContextPrecisionWithReference()
sample = SingleTurnSample(
    retrieved_contexts=["埃菲尔铁塔位于巴黎。"],
    reference_contexts=[
        "巴黎是法国的首都。",
        "埃菲尔铁塔是巴黎最著名的地标之一。"
    ]
)
score = await scorer.single_turn_ascore(sample)
```

### 1.2 上下文召回率（Context Recall）

**指标定义**：衡量成功检索到的相关文档（或信息片段）的数量。关注的是不遗漏重要信息，召回率越高表示遗漏的相关文档越少。由于召回率是关于不遗漏任何东西的，因此计算上下文召回率总是需要一个参考来比较。

**评分范围**：0.0-1.0，分数越高表示上下文覆盖率越好

**应用场景**：评估检索系统的完整性，确保重要信息不会被遗漏

**Ragas实现方式**：

#### 基于LLM的上下文召回率

| 实现方法           | Ragas类名            | 评估依据                                                  | 适用范围 |
| ------------------ | -------------------- | --------------------------------------------------------- | -------- |
| **标准方法** | `LLMContextRecall` | 将 `reference`分解为主张（claim）并验证是否被上下文支持 | 单轮对话 |

**计算公式**：

$$
\text{上下文召回率} = \frac{\text{检索到的上下文支持的参考中的主张数量}}{\text{参考中的主张总数}}
$$

#### 非基于LLM的上下文召回率

| 实现方法           | Ragas类名               | 评估依据                                                                  | 适用范围 |
| ------------------ | ----------------------- | ------------------------------------------------------------------------- | -------- |
| **标准方法** | `NonLLMContextRecall` | 使用非LLM字符串相似度比较 `retrieved_contexts`和 `reference_contexts` | 单轮对话 |

**计算公式**：

$$
\text{上下文召回率} = \frac{|\text{检索到的相关上下文数量}|}{|\text{参考上下文总数}|}
$$

**使用示例（基于LLM）**：

```python
from ragas.metrics import LLMContextRecall
from ragas import SingleTurnSample

scorer = LLMContextRecall(llm=evaluator_llm)
sample = SingleTurnSample(
    user_input="埃菲尔铁塔在哪里？",
    response="埃菲尔铁塔位于巴黎。",
    reference="埃菲尔铁塔位于巴黎。",
    retrieved_contexts=["巴黎是法国的首都。"]
)
score = await scorer.single_turn_ascore(sample)
```

**使用示例（非基于LLM）**：

```python
from ragas.metrics import NonLLMContextRecall
from ragas import SingleTurnSample

scorer = NonLLMContextRecall()
sample = SingleTurnSample(
    retrieved_contexts=["巴黎是法国的首都。"],
    reference_contexts=[
        "巴黎是法国的首都。",
        "埃菲尔铁塔是巴黎最著名的地标之一。"
    ]
)
score = await scorer.single_turn_ascore(sample)  # 输出: 0.5
```

### 1.3 上下文实体召回（Context Entity Recall）

**指标定义**：衡量检索到的上下文的召回率，计算方式是根据在 `reference` 和 `retrieved_contexts` 中都存在的实体数量与仅在 `reference` 中存在的实体数量的比例。简单来说，它衡量的是从 `reference` 中召回了多少比例的实体。这个指标在基于事实的用例中非常有用，例如旅游帮助台、历史问答等。

**评分范围**：0.0-1.0，分数越高表示实体召回率越好

**应用场景**：旅游帮助台、历史问答等事实类场景，命名实体识别任务的评估，知识图谱构建质量检查

**计算方式**：

使用两个集合：

- **RE**：参考中的实体集合
- **RCE**：检索到的上下文中的实体集合

**计算公式**：

$$
\text{上下文实体召回率} = \frac{\text{RCE 与 RE 之间的共同实体数量}}{\text{RE 中的实体总数}} = \frac{|RCE \cap RE|}{|RE|}
$$

**Ragas实现方式**：

| 实现方法           | Ragas类名               | 评估依据                                            | 适用范围 |
| ------------------ | ----------------------- | --------------------------------------------------- | -------- |
| **标准方法** | `ContextEntityRecall` | 从reference和retrieved_contexts中提取实体并计算重叠 | 单轮对话 |

**使用示例**：

```python
from ragas.metrics import ContextEntityRecall
from ragas import SingleTurnSample

scorer = ContextEntityRecall(llm=evaluator_llm)
sample = SingleTurnSample(
    reference="埃菲尔铁塔位于巴黎。",
    retrieved_contexts=["埃菲尔铁塔位于巴黎。"]
)
score = await scorer.single_turn_ascore(sample)
```

**计算示例说明**：

假设有以下内容：

- **参考**：泰姬陵是位于印度城市阿格拉亚穆纳河右岸的一座象牙白大理石陵墓。它由莫卧儿皇帝沙贾汗于1631年委托建造。
- **高实体召回上下文**：泰姬陵是位于印度阿格拉的建筑奇迹。它由莫卧儿皇帝沙贾汗建造，以纪念他心爱的妻子穆塔兹·玛哈尔。
- **低实体召回上下文**：泰姬陵是印度的一个标志性古迹。它是联合国教科文组织世界遗产。

计算步骤：

1. **参考中的实体 (RE)**：['泰姬陵', '亚穆纳河', '阿格拉', '1631', '沙贾汗', '穆塔兹·玛哈尔']
2. **上下文1中的实体 (RCE1)**：['泰姬陵', '阿格拉', '沙贾汗', '穆塔兹·玛哈尔', '印度']
3. **上下文2中的实体 (RCE2)**：['泰姬陵', '联合国教科文组织', '印度']

计算结果：

- 上下文1实体召回率 = |RCE1 ∩ RE| / |RE| = 4/6 = 0.666
- 上下文2实体召回率 = |RCE2 ∩ RE| / |RE| = 1/6 = 0.167

可以看到第一个上下文具有较高的实体召回率，因为它相对于参考具有更好的实体覆盖。

### 1.4 噪声敏感性（Noise Sensitivity）

**指标定义**：衡量系统在利用相关或不相关检索到的文档时，因提供错误回复而产生错误的频率。通过检查生成回复中的每个论断（statement），确定其是否根据"地面真实"（ground truth）是正确的，以及是否可以归因于相关（或不相关）检索到的上下文。

**评分范围**：0.0-1.0，分数越低表示对噪声越不敏感（抗噪性越强）

**应用场景**：

- 评估系统在嘈杂环境中的鲁棒性
- 测试检索系统对不相关信息的抵抗能力
- 优化数据预处理和噪声过滤策略

**计算公式**：

$$
\text{噪声敏感度} = \frac{|\text{回复中的错误论断总数}|}{|\text{回复中的论断总数}|}
$$

**Ragas实现方式**：

| 实现方法                 | Ragas类名                               | 评估模式                   | 适用范围 |
| ------------------------ | --------------------------------------- | -------------------------- | -------- |
| **相关噪声模式**   | `NoiseSensitivity(mode="relevant")`   | 评估相关上下文中的错误论断 | 单轮对话 |
| **不相关噪声模式** | `NoiseSensitivity(mode="irrelevant")` | 评估不相关上下文的影响     | 单轮对话 |

**使用示例（相关噪声模式）**：

```python
from ragas.metrics import NoiseSensitivity
from ragas import SingleTurnSample

scorer = NoiseSensitivity(llm=evaluator_llm, mode="relevant")
sample = SingleTurnSample(
    user_input="印度人寿保险公司（LIC）以什么闻名？",
    response="印度人寿保险公司（LIC）是印度最大的保险公司，以其庞大的投资组合而闻名。LIC为国家的金融稳定做出了贡献。",
    reference="印度人寿保险公司（LIC）是印度最大的保险公司，成立于1956年通过保险业国有化。它以管理庞大的投资组合而闻名。",
    retrieved_contexts=[
        "印度人寿保险公司（LIC）成立于1956年，是印度保险业国有化的结果。",
        "LIC是印度最大的保险公司，拥有庞大的投保人网络和巨额投资。",
        "作为印度最大的机构投资者，LIC管理着大量资金，为国家的金融稳定做出了贡献。",
        "印度经济是世界上增长最快的主要经济体之一，得益于金融、技术、制造业等行业。"
    ]
)
score = await scorer.single_turn_ascore(sample)  # 输出: 0.333
```

**使用示例（不相关噪声模式）**：

```python
scorer = NoiseSensitivity(llm=evaluator_llm, mode="irrelevant")
score = await scorer.single_turn_ascore(sample)
```

**计算示例说明**：

以上面的LIC案例为例，计算相关上下文的噪声敏感度：

**步骤1**：识别可以推断出地面真实（reference）的相关上下文

- 相关上下文包括前3个检索到的文档（关于LIC的信息）
- 不相关上下文是第4个（关于印度经济的信息）

**步骤2**：验证生成回复中的论断是否可以从相关上下文推断

- 回复包含3个论断：
  1. "LIC是印度最大的保险公司" ✓（可从上下文2推断）
  2. "以其庞大的投资组合而闻名" ✓（可从上下文2、3推断）
  3. "LIC为国家的金融稳定做出了贡献" ✗（地面真实未提及）

**步骤3**：识别错误论断

- 地面真实（reference）没有提及"LIC为国家的金融稳定做出贡献"
- 虽然这个信息在上下文3中存在，但不在地面真实中，因此是错误论断
- 错误论断数：1，总论断数：3

**步骤4**：计算噪声敏感度

$$
\text{噪声敏感度} = \frac{1}{3} = 0.333
$$

这表明回复中的三个论断有一个是错误的，系统对噪声有一定的敏感性。

**致谢**：噪声敏感度在 RAGChecker 中引入。

### 1.5 回答相关性（Response Relevancy）

**指标定义**：衡量回答与用户输入的关联程度。分数越高表示与用户输入越匹配，分数越低则表示回答不完整或包含冗余信息。如果回答直接且恰当地回应了原始问题，则被认为是相关的。此指标侧重于回答与问题意图的匹配程度，但不评估事实准确性。它会惩罚不完整或包含不必要细节的回答。

**评分范围**：0.0-1.0，分数越高表示相关性越强

**注意**：虽然分数通常介于0和1之间，但由于余弦相似度的数学范围是-1到1，因此无法保证总是如此。

**应用场景**：检测答非所问的情况，评估回答的针对性

**计算方法**：

该指标使用 `user_input` 和 `response` 按如下方式计算：

1. **生成问题**：基于回答生成一组人工问题（默认为3个）。这些问题旨在反映回答的内容。
2. **计算相似度**：计算用户输入嵌入 $(E_o)$ 与每个生成问题嵌入 $(E_{g_i})$ 之间的余弦相似度。
3. **取平均值**：对这些余弦相似度分数取平均值，得到回答相关性。

**计算公式**：

$$
\text{Answer Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \text{cosine similarity}(E_{g_i}, E_o)
$$

$$
\text{Answer Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \frac{E_{g_i} \cdot E_o}{\|E_{g_i}\| \|E_o\|}
$$

其中：

- $E_{g_i}$：第 $i$ 个生成问题的嵌入
- $E_o$：用户输入的嵌入
- $N$：生成问题的数量（默认为3）

**核心思想**：如果回答正确地回应了问题，那么很有可能仅凭回答就可以重构出原始问题。

**Ragas实现方式**：

| 实现方法           | Ragas类名             | 评估流程                                             | 适用范围 |
| ------------------ | --------------------- | ---------------------------------------------------- | -------- |
| **标准方法** | `ResponseRelevancy` | 基于response生成问题 → 计算与user_input的嵌入相似度 | 单轮对话 |

**使用示例**：

```python
from ragas.metrics import ResponseRelevancy
from ragas import SingleTurnSample

scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)
sample = SingleTurnSample(
    user_input="第一届超级碗是什么时候举行的？",
    response="第一届超级碗于1967年1月15日举行",
    retrieved_contexts=[
        "第一届AFL–NFL世界冠军赛是一场美式足球比赛，于1967年1月15日在洛杉矶纪念体育场举行。"
    ]
)
score = await scorer.single_turn_ascore(sample)  # 输出: 0.9165
```

**计算示例说明**：

**问题**：法国在哪里？首都是什么？

**低相关性回答**：法国位于西欧。

**高相关性回答**：法国位于西欧，巴黎是它的首都。

为了计算回答与给定问题的相关性，遵循两个步骤：

**步骤1**：使用LLM从生成的回答中反向生成'n'个问题变体

对于第一个回答（低相关性），LLM可能会生成：

- 问题1："法国位于欧洲的哪个部分？"
- 问题2："法国在欧洲处于什么地理位置？"
- 问题3："你能指出法国在欧洲的哪个区域吗？"

可以看到，这些生成的问题只涉及地理位置，未涉及首都。

**步骤2**：计算生成的问题与实际问题之间的平均余弦相似度

由于生成的问题只涵盖了原始问题的一部分内容（位置但不包括首都），余弦相似度会较低，表明回答不完整。

对于第二个回答（高相关性），生成的问题会同时涵盖位置和首都信息，因此相似度更高。

### 1.6 忠实度（Faithfulness）

**指标定义**：衡量 `response` 与 `retrieved_contexts` 的事实一致性。如果一个响应的所有主张（claims）都能得到检索到的上下文的支持，则该响应被认为是**忠实**的。通过将答案分解为独立的主张，然后验证每个主张是否被上下文支持来计算。

**评分范围**：0.0-1.0，分数越高表示答案越忠实于上下文

**应用场景**：确保智能体不产生幻觉或编造信息

**计算方法**：

1. **识别主张**：识别响应中的所有主张
2. **验证支持**：检查每个主张，看它是否可以从检索到的上下文中推断出来
3. **计算分数**：使用以下公式计算忠实度分数

**计算公式**：

$$
\text{忠实度分数} = \frac{\text{响应中得到检索到的上下文支持的主张数量}}{\text{响应中的主张总数}}
$$

**Ragas实现方式**：

| 实现方法               | Ragas类名                | 评估流程                      | 适用范围 |
| ---------------------- | ------------------------ | ----------------------------- | -------- |
| **标准LLM方法**  | `Faithfulness`         | 主张分解 → NLI验证           | 单轮对话 |
| **HHEM模型方法** | `FaithfulnesswithHHEM` | 使用Vectara的T5分类器检测幻觉 | 单轮对话 |

**类似指标（Nvidia高效方案）**：

| 指标名称             | Ragas类名                | 特点                           | 适用场景           |
| -------------------- | ------------------------ | ------------------------------ | ------------------ |
| **响应基础度** | `ResponseGroundedness` | 双重LLM判断机制，token效率更高 | 需要快速评估的场景 |

**注意**：`ResponseGroundedness` 不是 `Faithfulness` 的变体，而是来自Nvidia TruLens的独立指标，评估粒度不同。

**使用示例（标准LLM方法）**：

```python
from ragas.metrics import Faithfulness
from ragas import SingleTurnSample

scorer = Faithfulness(llm=evaluator_llm)
sample = SingleTurnSample(
    user_input="第一届超级碗是什么时候举行的？",
    response="第一届超级碗于1967年1月15日举行",
    retrieved_contexts=[
        "第一届AFL–NFL世界冠军赛是一场美式足球比赛，于1967年1月15日在洛杉矶纪念体育场举行。"
    ]
)
score = await scorer.single_turn_ascore(sample)  # 输出: 1.0
```

**使用示例（HHEM高效方法）**：

`FaithfulnesswithHHEM` 使用 Vectara 的 HHEM-2.1-Open 模型，这是一个分类模型（T5），经过训练用于检测LLM生成文本中的幻觉。该模型免费、体积小且开源，在生产用例中效率非常高。

```python
from ragas.metrics import FaithfulnesswithHHEM

scorer = FaithfulnesswithHHEM(llm=evaluator_llm)
sample = SingleTurnSample(
    user_input="第一届超级碗是什么时候举行的？",
    response="第一届超级碗于1967年1月15日举行",
    retrieved_contexts=[
        "第一届AFL–NFL世界冠军赛是一场美式足球比赛，于1967年1月15日在洛杉矶纪念体育场举行。"
    ]
)
score = await scorer.single_turn_ascore(sample)
```

您可以通过设置 `device` 参数将模型加载到指定设备上，并使用 `batch_size` 参数调整推理的批处理大小。默认情况下，模型加载在CPU上，批处理大小为10：

```python
scorer = FaithfulnesswithHHEM(
    llm=evaluator_llm,
    device="cuda:0",
    batch_size=10
)
score = await scorer.single_turn_ascore(sample)
```

**计算示例说明**：

**问题**：爱因斯坦在哪里出生？何时出生？

**上下文**：阿尔伯特·爱因斯坦（生于1879年3月14日）是一位德裔理论物理学家，被广泛认为是史上最伟大、最具影响力的科学家之一。

**高忠实度答案**：爱因斯坦于1879年3月14日出生在德国。

**低忠实度答案**：爱因斯坦于1879年3月20日出生在德国。

让我们看看如何使用低忠实度答案计算忠实度：

**步骤1**：将生成的答案分解成独立的主张

- 主张1："爱因斯坦出生在德国。"
- 主张2："爱因斯坦于1879年3月20日出生。"

**步骤2**：对于每个生成的主张，验证是否可以从给定上下文中推断出来

- 主张1：是 ✓（上下文说"德裔理论物理学家"）
- 主张2：否 ✗（上下文说3月14日，不是3月20日）

**步骤3**：使用公式计算忠实度

$$
\text{忠实度} = \frac{1}{2} = 0.5
$$

这表明响应中有50%的主张是忠实于上下文的，另外50%包含了幻觉（错误的日期）。

## 2. Nvidia高效指标

Nvidia高效指标来自Nvidia TruLens项目，这些指标专门设计用于提高评估效率和降低token成本。它们采用双重LLM判断机制，使用0/1/2或0/2/4的离散评分，相比传统方法具有更高的token效率。这些指标适合需要快速评估或大规模评估的生产场景。

### 2.1 答案准确性（Answer Accuracy）

**指标定义**：评估生成的答案与参考答案的一致性程度。通过两个独立的LLM判断调用，每个返回0、2或4的评分，然后归一化并取平均值。

**评分范围**：0.0-1.0，分数越高表示答案越准确

**评分标准**：

- **0** → 答案完全不正确或与参考答案完全不符
- **2** → 答案部分正确，包含部分准确信息
- **4** → 答案完全正确，与参考答案高度一致

**应用场景**：

- 需要快速评估答案质量的场景
- 大规模批量评估
- 生产环境中的实时质量监控
- Token成本敏感的应用

**计算方法**：

**步骤1**：使用两个不同的提示模板提示LLM，评估生成答案与参考答案的一致性。每个提示返回相关性评分**0**、**2**或**4**。

**步骤2**：每个评分通过除以4归一化到[0,1]的比例（即，0变为0.0，2变为0.5，4变为1.0）。如果两个评分都有效，则最终分数为这些归一化值的平均值；如果只有一个有效，则使用该评分。

**Ragas实现方式**：

| 实现方法           | Ragas类名          | 特点                       | 适用范围 |
| ------------------ | ------------------ | -------------------------- | -------- |
| **标准方法** | `AnswerAccuracy` | 双重LLM判断，0/2/4离散评分 | 单轮对话 |

**使用示例**：

```python
from ragas.metrics import AnswerAccuracy
from ragas import SingleTurnSample

scorer = AnswerAccuracy(llm=evaluator_llm)
sample = SingleTurnSample(
    user_input="阿尔伯特·爱因斯坦什么时候出生？",
    response="阿尔伯特·爱因斯坦出生于1879年。",
    reference="阿尔伯特·爱因斯坦出生于1879年3月14日。"
)
score = await scorer.single_turn_ascore(sample)  # 输出: 0.5 (部分正确，缺少具体日期)
```

**与Answer Correctness的对比**：

| 维度                  | Answer Accuracy (Nvidia)       | Answer Correctness (标准)         |
| --------------------- | ------------------------------ | --------------------------------- |
| **LLM调用次数** | 2次（每个评委一次）            | 多次（分解+验证+相似度）          |
| **Token使用**   | 低（仅输出离散分数）           | 高（生成详细推理）                |
| **可解释性**    | 低（原始分数，无推理）         | 高（提供详细的TP/FP/FN分析）      |
| **效率**        | 高（轻量级，适合小模型）       | 中（需要更多计算）                |
| **评估维度**    | 整体一致性判断                 | 事实准确性(75%) + 语义相似度(25%) |
| **适用场景**    | 快速评估、大规模部署、成本敏感 | 详细分析、质量深度评估            |

### 2.2 上下文相关性（Context Relevance）

**指标定义**：评估检索到的上下文（块或段落）是否与用户输入相关。通过两个独立的LLM判断调用，每个返回0、1或2的评分，然后归一化并取平均值。

**评分范围**：0.0-1.0，分数越高表示上下文相关性越强

**评分标准**：

- **0** → 检索到的上下文与用户查询完全不相关
- **1** → 上下文部分相关
- **2** → 上下文完全相关

**应用场景**：

- 评估检索系统的相关性
- 快速验证检索质量
- 实时检索效果监控
- Token成本敏感的场景

**计算方法**：

**步骤1**：使用两个不同的模板（template_relevance1和template_relevance2）提示LLM，评估检索到的上下文与用户查询的相关性。每个提示返回相关性评分**0**、**1**或**2**。

**步骤2**：每个评分通过除以2归一化到[0,1]的比例。如果两个评分都有效，则最终分数为这些归一化值的平均值；如果只有一个有效，则使用该评分。

**Ragas实现方式**：

| 实现方法           | Ragas类名            | 特点                   | 适用范围 |
| ------------------ | -------------------- | ---------------------- | -------- |
| **标准方法** | `ContextRelevance` | 双重LLM判断，0/1/2评分 | 单轮对话 |

**使用示例**：

```python
from ragas.metrics import ContextRelevance
from ragas import SingleTurnSample

scorer = ContextRelevance(llm=evaluator_llm)
sample = SingleTurnSample(
    user_input="阿尔伯特·爱因斯坦何时何地出生？",
    retrieved_contexts=[
        "阿尔伯特·爱因斯坦出生于1879年3月14日。",
        "阿尔伯特·爱因斯坦出生于德国符腾堡的乌尔姆。",
    ]
)
score = await scorer.single_turn_ascore(sample)  # 输出: 1.0
```

**计算示例说明**：

在上面的例子中：

- **用户输入**："阿尔伯特·爱因斯坦何时何地出生？"
- **检索到的上下文**：
  - 上下文1："阿尔伯特·爱因斯坦出生于1879年3月14日。"（回答"何时"）
  - 上下文2："阿尔伯特·爱因斯坦出生于德国符腾堡的乌尔姆。"（回答"何地"）

两个上下文共同完全回答了用户的查询。因此，两个LLM判断都会将相关性评分定为**2**（完全相关）。每个评分归一化后得到**1.0** (2/2)，取平均值后，最终的上下文相关性得分为**1.0**。

**与Context Precision和Context Recall的对比**：

| 维度                  | Context Relevance (Nvidia) | Context Precision (标准) | Context Recall (标准) |
| --------------------- | -------------------------- | ------------------------ | --------------------- |
| **LLM调用次数** | 2次（双判断机制）          | 1次（验证有用性）        | 1次（验证归因）       |
| **Token使用**   | 低（离散评分）             | 高（详细推理）           | 高（详细推理）        |
| **可解释性**    | 低（原始分数）             | 高（逐块分析）           | 高（逐句归因）        |
| **鲁棒性**      | 高（双重验证）             | 中（单次判断）           | 中（单次判断）        |
| **评估焦点**    | 内容是否相关               | 相关文档排名质量         | 是否遗漏重要信息      |

### 2.3 响应基础度（Response Groundedness）

**指标定义**：衡量响应在多大程度上得到检索到的上下文的支持或"基于"检索到的上下文。评估响应中的每个论断是否可以在提供的上下文中找到。

**评分范围**：0.0-1.0，分数越高表示响应越基于上下文

**评分标准**：

- **0** → 响应完全不基于上下文
- **1** → 响应部分基于上下文
- **2** → 响应完全基于上下文（每个语句都可以在检索到的上下文中找到或从中推断出来）

**应用场景**：

- 快速幻觉检测
- 实时响应质量监控
- 大规模生成内容验证
- 成本敏感的生产环境

**计算方法**：

**步骤1**：使用两个不同的模板提示LLM，评估响应相对于检索到的上下文的基础度。每个提示返回基础度评分**0**、**1**或**2**。

**步骤2**：每个评分通过除以2归一化到[0,1]的比例（即，0变为0.0，1变为0.5，2变为1.0）。如果两个评分都有效，则最终分数为这些归一化值的平均值；如果只有一个有效，则使用该评分。

**Ragas实现方式**：

| 实现方法           | Ragas类名                | 特点                   | 适用范围 |
| ------------------ | ------------------------ | ---------------------- | -------- |
| **标准方法** | `ResponseGroundedness` | 双重LLM判断，0/1/2评分 | 单轮对话 |

**使用示例**：

```python
from ragas.metrics import ResponseGroundedness
from ragas import SingleTurnSample

scorer = ResponseGroundedness(llm=evaluator_llm)
sample = SingleTurnSample(
    response="阿尔伯特·爱因斯坦出生于1879年。",
    retrieved_contexts=[
        "阿尔伯特·爱因斯坦出生于1879年3月14日。",
        "阿尔伯特·爱因斯坦出生于德国符腾堡的乌尔姆。",
    ]
)
score = await scorer.single_turn_ascore(sample)  # 输出: 1.0
```

**计算示例说明**：

在上面的例子中：

- **响应**："阿尔伯特·爱因斯坦出生于1879年。"
- **检索到的上下文**：
  - 上下文1："阿尔伯特·爱因斯坦出生于1879年3月14日。"
  - 上下文2："阿尔伯特·爱因斯坦出生于德国符腾堡的乌尔姆。"

响应的论断（出生于1879年）得到上下文的完全支持，即使日期是部分提供的。两个LLM判断都会将基础度评分定为**2**（完全基于上下文）。归一化后得到**1.0** (2/2)，取平均值后，最终的响应基础度得分为**1.0**。

**与Faithfulness的对比**：

| 维度                  | Response Groundedness (Nvidia) | Faithfulness (标准)        |
| --------------------- | ------------------------------ | -------------------------- |
| **LLM调用次数** | 2次（双判断机制）              | 2次（分解+验证）           |
| **Token使用**   | 低（离散评分）                 | 高（详细推理）             |
| **可解释性**    | 低（原始分数）                 | 高（逐个主张透明推理）     |
| **评估粒度**    | 整体基础度判断                 | 细粒度主张级别分析         |
| **鲁棒性**      | 高（双重验证）                 | 高（结合用户输入全面评估） |
| **适用场景**    | 快速幻觉检测、成本敏感         | 详细幻觉分析、高质量要求   |

## 3. 智能体或工具使用案例

智能体或工具使用工作流程可以在多个维度上进行评估。以下指标用于评估智能体或工具在给定任务中的性能，特别是在多轮对话和复杂任务中的表现。

### 3.1 主题一致性（Topic Adherence）

**指标定义**：评估AI在交互过程中保持在预定义领域内的能力。部署在实际应用中的AI系统应遵守特定的领域，但LLM有时可能会忽略此限制而回答通用查询。此指标在会话式AI系统中尤为重要，因为AI预期仅为与预定义领域相关的查询提供帮助。

**评分范围**：0.0-1.0，分数越高表示主题相关性越强

**应用场景**：

- 会话式AI系统的领域遵守性评估
- 内容审查和合规性检查
- 教育内容质量评估
- 客服服务质量控制

**计算方式**：

`TopicAdherenceScore` 需要AI系统应遵守的预定义主题集合，该集合通过 `reference_topics` 与 `user_input` 一起提供。此指标可以计算主题一致性的精度（precision）、召回率（recall）和F1分数：

**精确率（Precision）**：

$$
\text{Precision} = \frac{|\text{回答且符合主题的查询}|}{|\text{回答且符合主题的查询}| + |\text{回答但不符合主题的查询}|}
$$

精确率衡量AI回答的问题中有多少比例坚持了主题（避免跑题）。

**召回率（Recall）**：

$$
\text{Recall} = \frac{|\text{回答且符合主题的查询}|}{|\text{回答且符合主题的查询}| + |\text{拒绝但应该回答的查询}|}
$$

召回率衡量预定义主题被覆盖的比例（覆盖全面）。

**F1分数**：

$$
\text{F1 Score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**Ragas实现方式**：

| 实现方法             | Ragas类名                                 | 评估模式                               | 适用范围 |
| -------------------- | ----------------------------------------- | -------------------------------------- | -------- |
| **精确率模式** | `TopicAdherenceScore(mode="precision")` | 回答的问题中坚持主题的比例（避免跑题） | 多轮对话 |
| **召回率模式** | `TopicAdherenceScore(mode="recall")`    | 预定义主题被覆盖的比例（覆盖全面）     | 多轮对话 |
| **F1模式**     | `TopicAdherenceScore(mode="f1")`        | 精确率和召回率的调和平均               | 多轮对话 |

**使用示例（精确率模式）**：

```python
from ragas.metrics import TopicAdherenceScore
from ragas.dataset_schema import MultiTurnSample
from ragas.messages import HumanMessage, AIMessage, ToolCall, ToolMessage

sample_input = [
    HumanMessage(content="给我讲讲爱因斯坦的相对论"),
    AIMessage(content="广义相对论解释了引力..."),
    HumanMessage(content="你知道有什么巧克力蛋糕食谱吗？"),
    AIMessage(content="感谢您的关注，但我专门设计用来帮助解决科学相关问题。还有什么关于物理学或爱因斯坦的问题吗？"),
    HumanMessage(content="光速是多少？"),
    AIMessage(content="真空中的光速约为每秒299,792,458米。"),
    HumanMessage(content="你能推荐一部好电影吗？"),
    AIMessage(content="这是一个很受欢迎的巧克力蛋糕食谱：配料包括面粉、糖、可可粉、鸡蛋、牛奶和黄油。", tool_calls=[
        ToolCall(name="recipe_search", args={"query": "巧克力蛋糕食谱"})
    ]),
    ToolMessage(content="这是一个很受欢迎的巧克力蛋糕食谱：配料包括面粉、糖、可可粉、鸡蛋、牛奶和黄油。说明：混合干配料，加入湿配料，在350°F下烘烤30-35分钟。"),
    AIMessage(content="我找到了一个很棒的巧克力蛋糕食谱！您想要完整的细节，还是这个摘要就够了？")
]

sample = MultiTurnSample(user_input=sample_input, reference_topics=["science"])
scorer = TopicAdherenceScore(llm=evaluator_llm, mode="precision")
await scorer.multi_turn_ascore(sample)  # 输出: 0.6666666666444444
```

要将模式更改为召回率，请将 `mode` 参数设置为 `recall`：

```python
scorer = TopicAdherenceScore(llm=evaluator_llm, mode="recall")
await scorer.multi_turn_ascore(sample)  # 输出: 0.99999999995
```

**计算示例说明**：

在上面的示例中，AI系统的主题是"science"（科学）：

1. **用户问题分类**：

   - 问题1："Einstein的相对论" → 科学相关 ✓
   - 问题2："巧克力蛋糕食谱" → 非科学相关 ✗
   - 问题3："光速是多少" → 科学相关 ✓
   - 问题4："推荐一部好电影" → 非科学相关 ✗
2. **AI响应分类**：

   - 响应1：回答了相对论（科学） → 回答且符合主题 ✓
   - 响应2：拒绝了蛋糕食谱，引导回科学话题 → 正确拒绝 ✓
   - 响应3：回答了光速（科学） → 回答且符合主题 ✓
   - 响应4：回答了巧克力蛋糕食谱 → 回答但不符合主题 ✗
3. **精确率计算**（避免跑题）：

   - 回答且符合主题：2个（相对论、光速）
   - 回答但不符合主题：1个（蛋糕食谱）
   - Precision = 2 / (2 + 1) = 0.6667
4. **召回率计算**（覆盖全面）：

   - 回答且符合主题：2个
   - 拒绝但应该回答的：0个（科学问题都被回答了）
   - Recall = 2 / (2 + 0) ≈ 1.0

这说明AI在覆盖科学主题方面做得很好（召回率高），但偶尔会回答非科学问题（精确率较低）。

### 3.2 工具调用准确性（Tool Call Accuracy）

**指标定义**：评估LLM在识别和调用完成给定任务所需工具方面的性能。通过比较AI的工具调用与参考工具调用来计算分数，默认要求按顺序匹配。值范围在0到1之间，值越高表示性能越好。

**评分范围**：0.0-1.0，综合序列对齐和参数准确度

**应用场景**：

- API调用自动化测试
- 智能体工具使用能力评估
- 多步骤任务执行质量检查

**计算方式**：

`ToolCallAccuracy` 需要 `user_input` 和 `reference_tool_calls` 来评估LLM的工具调用性能。`reference_tool_calls` 中指定的工具调用序列被用作理想结果。如果AI进行的工具调用与 `reference_tool_calls` 的顺序或序列不匹配，该指标将返回较低分数。这有助于确保AI能够按正确的顺序识别和调用所需工具来完成给定任务。

**Ragas实现方式**：

| 实现方法           | Ragas类名              | 评估维度              | 适用范围 |
| ------------------ | ---------------------- | --------------------- | -------- |
| **标准方法** | `ToolCallAccuracy()` | 序列对齐 + 参数准确度 | 多轮对话 |

**自定义参数比较**：

默认情况下，工具名称和参数使用精确字符串匹配进行比较。但有时这可能不是最优的，例如当参数是自然语言字符串时。您可以使用任何ragas指标（值介于0和1之间）作为距离度量：

```python
from ragas.metrics import ToolCallAccuracy
from ragas.metrics._string import NonLLMStringSimilarity

metric = ToolCallAccuracy()
metric.arg_comparison_metric = NonLLMStringSimilarity()  # 自定义参数比较方式
```

**使用示例**：

```python
from ragas.metrics import ToolCallAccuracy
from ragas.dataset_schema import MultiTurnSample
from ragas.messages import HumanMessage, AIMessage, ToolCall, ToolMessage

sample = MultiTurnSample(
    user_input=[
        HumanMessage(content="纽约现在的天气怎么样？"),
        AIMessage(
            content="纽约目前的温度是75°F，部分多云。",
            tool_calls=[ToolCall(name="weather_check", args={"location": "纽约"})]
        ),
        HumanMessage(content="你能把它转换成摄氏度吗？"),
        AIMessage(
            content="让我帮你转换成摄氏度。",
            tool_calls=[ToolCall(name="temperature_conversion", args={"temperature_fahrenheit": 75})]
        ),
        ToolMessage(content="75°F约为23.9°C。"),
        AIMessage(content="75°F约为23.9°C。")
    ],
    reference_tool_calls=[
        ToolCall(name="weather_check", args={"location": "纽约"}),
        ToolCall(name="temperature_conversion", args={"temperature_fahrenheit": 75})
    ]
)

scorer = ToolCallAccuracy()
score = await scorer.multi_turn_ascore(sample)  # 输出: 1.0
```

### 3.3 智能体目标准确性（Agent Goal Accuracy）

**指标定义**：评估LLM在识别和实现用户目标方面的性能。这是一个二元指标，1表示AI已经实现目标，0表示AI没有实现目标。

**评分范围**：0.0 或 1.0（二元：达成/未达成）

**应用场景**：

- 复杂任务完成度评估
- 智能体目标理解和执行能力测试
- 多轮对话任务效果检查

**Ragas实现方式**：

| 实现方法         | Ragas类名                             | 评估依据                          | 适用范围 |
| ---------------- | ------------------------------------- | --------------------------------- | -------- |
| **有参考** | `AgentGoalAccuracyWithReference`    | 从对话中推断目标，与reference比较 | 多轮对话 |
| **无参考** | `AgentGoalAccuracyWithoutReference` | 从对话中推断目标和结果，自动判断  | 多轮对话 |

#### 有参考（With Reference）

计算有参考的 `AgentGoalAccuracyWithReference` 需要 `user_input` 和 `reference` 来评估LLM在识别和实现用户目标方面的性能。带注释的 `reference` 将用作理想结果。该指标通过比较 `reference` 与工作流程结束时实现的目标来计算。

**使用示例（有参考）**：

```python
from ragas.metrics import AgentGoalAccuracyWithReference
from ragas.dataset_schema import MultiTurnSample
from ragas.messages import HumanMessage, AIMessage, ToolCall, ToolMessage

scorer = AgentGoalAccuracyWithReference(llm=evaluator_llm)
sample = MultiTurnSample(
    user_input=[
        HumanMessage(content="嘿，帮我在最近最好的中餐厅预订晚上8点的桌子"),
        AIMessage(
            content="好的，让我帮你找最好的选择。",
            tool_calls=[ToolCall(name="restaurant_search", args={"cuisine": "中餐", "time": "8:00pm"})]
        ),
        ToolMessage(content="找到几个选择：1. 金龙餐厅，2. 翡翠宫"),
        AIMessage(content="我找到了一些很棒的选择：金龙餐厅和翡翠宫。你更喜欢哪一个？"),
        HumanMessage(content="就选金龙餐厅吧。"),
        AIMessage(
            content="很好的选择！我会为你在金龙餐厅预订晚上8点的桌子。",
            tool_calls=[ToolCall(name="restaurant_book", args={"name": "金龙餐厅", "time": "8:00pm"})]
        ),
        ToolMessage(content="已在金龙餐厅预订晚上8点的桌子。"),
        AIMessage(content="你在金龙餐厅晚上8点的桌子已预订好。祝你用餐愉快！"),
        HumanMessage(content="谢谢"),
    ],
    reference="已在其中一家中餐厅预订晚上8点的桌子"
)
score = await scorer.multi_turn_ascore(sample)  # 输出: 1.0
```

#### 无参考（Without Reference）

`AgentGoalAccuracyWithoutReference` 在无参考模式下工作，该指标将评估LLM在没有任何参考的情况下识别和实现用户目标方面的性能。在此模式下，期望结果是从工作流程中的人工交互推断出来的。

**使用示例（无参考）**：

```python
from ragas.metrics import AgentGoalAccuracyWithoutReference
from ragas.dataset_schema import MultiTurnSample
from ragas.messages import HumanMessage, AIMessage, ToolCall, ToolMessage

scorer = AgentGoalAccuracyWithoutReference(llm=evaluator_llm)
sample = MultiTurnSample(
    user_input=[
        HumanMessage(content="嘿，帮我在最近最好的中餐厅预订晚上8点的桌子"),
        AIMessage(
            content="好的，让我帮你找最好的选择。",
            tool_calls=[ToolCall(name="restaurant_search", args={"cuisine": "中餐", "time": "8:00pm"})]
        ),
        ToolMessage(content="找到几个选择：1. 金龙餐厅，2. 翡翠宫"),
        AIMessage(content="我找到了一些很棒的选择：金龙餐厅和翡翠宫。你更喜欢哪一个？"),
        HumanMessage(content="就选金龙餐厅吧。"),
        AIMessage(
            content="很好的选择！我会为你在金龙餐厅预订晚上8点的桌子。",
            tool_calls=[ToolCall(name="restaurant_book", args={"name": "金龙餐厅", "time": "8:00pm"})]
        ),
        ToolMessage(content="已在金龙餐厅预订晚上8点的桌子。"),
        AIMessage(content="你在金龙餐厅晚上8点的桌子已预订好。祝你用餐愉快！"),
        HumanMessage(content="谢谢"),
    ]
)
score = await scorer.multi_turn_ascore(sample)  # 输出: 1.0
```

## 4. 模型通用指标

模型通用指标适用于各种AI模型的评估，不特定于某个应用领域。

### 4.1 文本相似度指标

#### 4.1.1 语义相似度（Semantic Similarity）

**指标定义**：基于嵌入向量的语义相似度计算。使用双编码器模型计算response和reference的余弦相似度。

**评分范围**：0.0-1.0，分数越高表示语义越相似

**应用场景**：

- 答案质量评估
- 语义等价性检测
- 内容去重和聚类

**Ragas实现方式**：

| 实现方法           | Ragas类名              | 计算方式             | 适用范围 |
| ------------------ | ---------------------- | -------------------- | -------- |
| **标准方法** | `SemanticSimilarity` | 向量化 → 余弦相似度 | 单轮对话 |

**使用示例**：

```python
from ragas.metrics import SemanticSimilarity
from ragas import SingleTurnSample

scorer = SemanticSimilarity(embeddings=evaluator_embeddings)
sample = SingleTurnSample(
    response="埃菲尔铁塔位于巴黎。",
    reference="埃菲尔铁塔在巴黎。它的高度为1000英尺。"
)
score = await scorer.single_turn_ascore(sample)
```

### 4.2 n-gram重叠指标

#### 4.2.1 BLEU分数

**指标定义**：基于n-gram精确率和简洁惩罚评估response质量，最初设计用于机器翻译。

**评分范围**：0.0-1.0，分数越高表示翻译质量越好

**应用场景**：机器翻译质量评估、文本生成评估

**Ragas实现方式**：

| 实现方法           | Ragas类名       | 计算依据                  | 特点         |
| ------------------ | --------------- | ------------------------- | ------------ |
| **标准方法** | `BleuScore()` | sacrebleu库，n-gram精确率 | 关注精确匹配 |

**使用示例**：

```python
from ragas.metrics import BleuScore
from ragas import SingleTurnSample

scorer = BleuScore()
sample = SingleTurnSample(
    response="埃菲尔铁塔位于印度。",
    reference="埃菲尔铁塔位于巴黎。"
)
score = await scorer.single_turn_ascore(sample)
```

#### 4.2.2 ROUGE分数

**指标定义**：基于n-gram召回率、精确率和F1分数衡量response和reference的重叠。

**评分范围**：0.0-1.0，分数越高表示重叠度越好

**应用场景**：文本摘要质量评估、机器翻译系统比较、内容生成质量检查

**Ragas实现方式**：

| 实现方法           | Ragas类名        | 可选配置                               | 说明         |
| ------------------ | ---------------- | -------------------------------------- | ------------ |
| **标准方法** | `RougeScore()` | rouge_type: "rouge1"/"rouge2"/"rougeL" | 默认rouge2   |
|                    |                  | mode: "recall"/"precision"/"fmeasure"  | 默认fmeasure |

**使用示例**：

```python
from ragas.metrics import RougeScore

scorer = RougeScore(rouge_type="rougeL", mode="recall")
score = await scorer.single_turn_ascore(sample)
```

#### 4.2.3 CHRF分数

**指标定义**：基于字符n-gram的评估指标，对形态丰富的语言更友好。

**评分范围**：0.0-1.0，分数越高表示相似度越好

**应用场景**：多语言文本生成评估、字符级别质量检查

**Ragas实现方式**：

| 实现方法           | Ragas类名       | 特点         | 优势             |
| ------------------ | --------------- | ------------ | ---------------- |
| **标准方法** | `ChrfScore()` | 字符级n-gram | 对形态变化不敏感 |

**使用示例**：

```python
from ragas.metrics import ChrfScore
from ragas import SingleTurnSample

scorer = ChrfScore()
sample = SingleTurnSample(
    response="埃菲尔铁塔位于印度。",
    reference="埃菲尔铁塔位于巴黎。"
)
score = await scorer.single_turn_ascore(sample)
```

### 4.3 字符串匹配指标

#### 4.3.1 精确匹配（Exact Match）

**指标定义**：严格的字符串完全匹配评估。检查response是否与reference完全相同。

**评分范围**：0或1（二元）

**应用场景**：结构化数据提取评估、格式化输出检查、确定性答案验证

**Ragas实现方式**：

| 实现方法           | Ragas类名        | 计算方式           | 特点         |
| ------------------ | ---------------- | ------------------ | ------------ |
| **标准方法** | `ExactMatch()` | 字符串完全相等判断 | 最严格的匹配 |

**使用示例**：

```python
from ragas.metrics import ExactMatch
from ragas import SingleTurnSample

scorer = ExactMatch()
sample = SingleTurnSample(
    response="巴黎",
    reference="巴黎"
)
score = await scorer.single_turn_ascore(sample)  # 输出: 1.0
```

#### 4.3.2 字符串存在性（String Presence）

**指标定义**：检查response是否包含reference文本。

**评分范围**：0或1（二元）

**应用场景**：关键信息提取验证、格式合规性检查、内容过滤和审核

**Ragas实现方式**：

| 实现方法           | Ragas类名            | 计算方式     | 特点       |
| ------------------ | -------------------- | ------------ | ---------- |
| **标准方法** | `StringPresence()` | 子串包含判断 | 宽松的匹配 |

**使用示例**：

```python
from ragas.metrics import StringPresence

scorer = StringPresence()
sample = SingleTurnSample(
    response="埃菲尔铁塔位于巴黎。",
    reference="埃菲尔铁塔"
)
score = await scorer.single_turn_ascore(sample)  # 输出: 1.0
```

#### 4.3.3 字符串相似度（Non-LLM String Similarity）

**指标定义**：使用传统字符串距离度量（如Levenshtein、Hamming、Jaro）衡量response和reference的相似度。

**评分范围**：0.0-1.0，分数越高表示越相似

**应用场景**：快速相似度评估、不依赖LLM的场景、成本敏感场景

**Ragas实现方式**：

| 实现方法                      | Ragas类名                                                            | 距离度量         | 特点           |
| ----------------------------- | -------------------------------------------------------------------- | ---------------- | -------------- |
| **Levenshtein（默认）** | `NonLLMStringSimilarity()`                                         | 编辑距离         | 通用性强       |
| **Hamming距离**         | `NonLLMStringSimilarity(distance_measure=DistanceMeasure.HAMMING)` | 位置对应字符差异 | 适合等长字符串 |
| **Jaro距离**            | `NonLLMStringSimilarity(distance_measure=DistanceMeasure.JARO)`    | 字符匹配和转置   | 适合短字符串   |

**使用示例**：

```python
from ragas.metrics._string import NonLLMStringSimilarity, DistanceMeasure

scorer = NonLLMStringSimilarity()
sample = SingleTurnSample(
    response="埃菲尔铁塔位于印度。",
    reference="埃菲尔铁塔位于巴黎。"
)
score = await scorer.single_turn_ascore(sample)  # 输出: ~0.89

# 使用Hamming距离
scorer_hamming = NonLLMStringSimilarity(distance_measure=DistanceMeasure.HAMMING)
```

### 4.4 自定义评分指标

#### 4.4.1 方面批评（Aspect Critic）

**指标定义**：基于自由形式自然语言定义的预定义方面来评估响应。使用多数投票机制，输出二元结果（是/否）。

**评分范围**：0或1（二元）

**应用场景**：多维度质量评估、定制化评分体系、专业领域质量检查（如有害性、恶意性等）

**Ragas实现方式**：

| 实现方法           | Ragas类名                                  | 评估方式               | 特点       |
| ------------------ | ------------------------------------------ | ---------------------- | ---------- |
| **标准方法** | `AspectCritic(name=..., definition=...)` | 多次LLM判断 + 多数投票 | 高度可定制 |

**使用示例**：

```python
from ragas.metrics import AspectCritic
from ragas import SingleTurnSample

scorer = AspectCritic(
    name="maliciousness",
    definition="提交的内容是否意图伤害、欺骗或利用用户？",
    llm=evaluator_llm
)
sample = SingleTurnSample(
    user_input="埃菲尔铁塔在哪里？",
    response="埃菲尔铁塔位于巴黎。"
)
score = await scorer.single_turn_ascore(sample)  # 输出: 0（不恶意）
```

#### 4.4.2 简单标准评分（Simple Criteria Scoring）

**指标定义**：基于预定义的单一自由形式评分标准对响应进行整数评分。

**评分范围**：标准中指定的整数范围（如0-5）

**应用场景**：需要多级评分的场景，如内容质量评估

**Ragas实现方式**：

| 实现方法           | Ragas类名                                         | 评估方式      | 特点           |
| ------------------ | ------------------------------------------------- | ------------- | -------------- |
| **标准方法** | `SimpleCriteriaScore(name=..., definition=...)` | LLM按标准打分 | 灵活的评分范围 |

**使用示例**：

```python
from ragas.metrics import SimpleCriteriaScore

scorer = SimpleCriteriaScore(
    name="quality_score",
    definition="根据答案质量和完整性评分0到5分",
    llm=evaluator_llm
)
score = await scorer.single_turn_ascore(sample)
```

#### 4.4.3 评分标准评分（Rubrics Based Scoring）

**指标定义**：基于用户定义的详细评分标准进行评估。每个分数都有明确的描述，确保一致和客观的评估。

**评分范围**：1-5（可自定义）

**应用场景**：需要详细评分说明的场景，如学术评估、内容审核

**Ragas实现方式**：

| 实现方法           | Ragas类名                       | 评估方式                   | 特点     |
| ------------------ | ------------------------------- | -------------------------- | -------- |
| **统一标准** | `RubricsScore(rubrics={...})` | 所有样本使用同一套评分标准 | 一致性强 |
| **实例特定** | `InstanceRubrics()`           | 每个样本可使用不同评分标准 | 灵活性强 |

**使用示例（统一标准）**：

```python
from ragas.metrics import RubricsScore

rubrics = {
    "score1_description": "完全不正确，未解决问题",
    "score2_description": "部分正确，但有重大错误",
    "score3_description": "基本正确，缺少部分细节",
    "score4_description": "正确清晰，仅有轻微瑕疵",
    "score5_description": "完全正确，清晰全面",
}

scorer = RubricsScore(rubrics=rubrics, llm=evaluator_llm)
score = await scorer.single_turn_ascore(sample)
```

### 4.5 专项领域指标

#### 4.5.1 摘要质量评分（Summarization Score）

**指标定义**：评估文本摘要质量的综合性指标。通过提取关键短语、生成问题、评估摘要回答能力来计算分数。

**评分范围**：0.0-1.0，分数越高表示摘要质量越好

**应用场景**：新闻摘要系统评估、学术论文摘要质量检查、会议纪要生成评估、文档自动摘要系统优化

**Ragas实现方式**：

| 实现方法           | Ragas类名                | 评估流程                                        | 特点     |
| ------------------ | ------------------------ | ----------------------------------------------- | -------- |
| **标准方法** | `SummarizationScore()` | 关键短语提取 → 问题生成 → QA评分 + 简洁性评分 | 综合评估 |

**使用示例**：

```python
from ragas.metrics import SummarizationScore
from ragas import SingleTurnSample

scorer = SummarizationScore(llm=evaluator_llm)
sample = SingleTurnSample(
    response="一款健身应用帮助用户跟踪运动、膳食和饮水量，并提供个性化建议。",
    reference_contexts=[
        "一家公司正在推出一款健身追踪应用，帮助用户设定运动目标、记录膳食、跟踪饮水量，提供个性化的锻炼建议和激励提醒。"
    ]
)
score = await scorer.single_turn_ascore(sample)
```

#### 4.5.2 SQL语义等价性（SQL Semantic Equivalence）

**指标定义**：评估生成的SQL查询与参考SQL在语义上是否等价。基于数据库schema进行逻辑分析。

**评分范围**：0或1（二元：等价/不等价）

**应用场景**：SQL生成系统评估、数据库查询优化验证、自然语言到SQL转换系统测试、数据分析工具质量评估

**Ragas实现方式**：

| 实现方法          | Ragas类名               | 评估依据                 | 所需输入                                            |
| ----------------- | ----------------------- | ------------------------ | --------------------------------------------------- |
| **LLM判断** | `LLMSQLEquivalence()` | LLM解释两个SQL并比较语义 | response + reference + reference_contexts（schema） |

**使用示例**：

```python
from ragas.metrics import LLMSQLEquivalence
from ragas import SingleTurnSample

scorer = LLMSQLEquivalence(llm=evaluator_llm)
sample = SingleTurnSample(
    response="""
        SELECT p.product_name, SUM(oi.quantity) AS total_quantity
        FROM order_items oi
        JOIN products p ON oi.product_id = p.product_id
        GROUP BY p.product_name;
    """,
    reference="""
        SELECT p.product_name, COUNT(oi.quantity) AS total_quantity
        FROM order_items oi
        JOIN products p ON oi.product_id = p.product_id
        GROUP BY p.product_name;
    """,
    reference_contexts=[
        "表 order_items: order_item_id INT, order_id INT, product_id INT, quantity INT",
        "表 products: product_id INT, product_name VARCHAR, price DECIMAL"
    ]
)
score = await scorer.single_turn_ascore(sample)
```

#### 4.5.3 数据比较评分（Data Comparison Score）

**指标定义**：比较两个数据集（通常是CSV格式表格数据）的相似度。基于DataCompy库实现。

**评分范围**：0.0-1.0，根据选择的度量类型而定

**应用场景**：数据质量验证、ETL流程结果验证、数据迁移准确性检查、数据同步一致性验证

**Ragas实现方式**：

| 实现方法         | Ragas类名                                          | 比较模式 | 度量类型            |
| ---------------- | -------------------------------------------------- | -------- | ------------------- |
| **行比较** | `DataCompyScore(mode="row", metric="precision")` | 按行比较 | precision/recall/f1 |
| **列比较** | `DataCompyScore(mode="column", metric="recall")` | 按列比较 | precision/recall/f1 |

**使用示例**：

```python
from ragas.metrics import DataCompyScore
from ragas import SingleTurnSample

data1_csv = """acct_id,dollar_amt,name
10000001234,123.45,George
10000001235,0.45,Michael
"""

data2_csv = """acct_id,dollar_amt,name
10000001234,123.4,George
10000001235,0.45,Michael
"""

scorer = DataCompyScore(mode="row", metric="f1")
sample = SingleTurnSample(response=data1_csv, reference=data2_csv)
score = await scorer.single_turn_ascore(sample)
```

## 5. 性能指标

性能指标主要关注智能体在实际应用中的效率和成本表现。

### 5.1 Token使用量跟踪

**指标定义**：跟踪和分析模型输入输出令牌的使用情况，用于成本监控和资源规划。

**关键指标**：

- 输入令牌数量（Input Tokens）
- 输出令牌数量（Output Tokens）
- 总令牌数量（Total Tokens）
- 模型类型识别

**应用场景**：API调用成本监控、模型使用量统计、资源规划和管理

**Ragas实现方式**：

Ragas会自动跟踪评估过程中的token使用。可以通过以下方式访问：

```python
from ragas import evaluate
from ragas.metrics import Faithfulness, ContextPrecision

result = evaluate(
    dataset=evaluation_dataset,
    metrics=[Faithfulness(), ContextPrecision()],
    llm=evaluator_llm
)

# 访问token使用统计
print(result.total_tokens())  # 总token数
print(result.token_usage_per_metric())  # 每个指标的token使用
```

**成本计算**：

```python
# 假设定价：输入$0.01/1K tokens，输出$0.03/1K tokens
input_tokens = result.total_tokens()['input_tokens']
output_tokens = result.total_tokens()['output_tokens']

cost = (input_tokens / 1000 * 0.01) + (output_tokens / 1000 * 0.03)
print(f"Total cost: ${cost:.4f}")
```

### 5.2 响应时间性能

虽然Ragas主要关注质量指标，但在实际智能体评估中，以下性能指标同样重要：

| 指标                              | 定义                     | 测量单位            | 评估方法         |
| --------------------------------- | ------------------------ | ------------------- | ---------------- |
| **延迟（Latency）**         | 从输入到输出的时间延迟   | 毫秒（ms）或秒（s） | 自行实现时间测量 |
| **吞吐量（Throughput）**    | 单位时间内处理的请求数量 | 请求/秒（RPS）      | 批量测试统计     |
| **并发性能（Concurrency）** | 同时处理多个请求的能力   | 并发用户数          | 压力测试工具     |

**注意**：这些指标需要在实际应用场景中单独测量，Ragas不直接提供。
